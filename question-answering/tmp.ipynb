{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "There was a problem when trying to write in your cache folder (/work/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-13 11:25:56,592] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import T5Tokenizer,MT5ForConditionalGeneration,MT5Config\n",
    "max_knowledge_length=480\n",
    "max_seq_length=512\n",
    "max_target_length=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pretrain_path = '/home/wangran108/tmp/tst-summarization/'\n",
    "config = MT5Config.from_pretrained(pretrain_path)\n",
    "tokenizer=T5Tokenizer.from_pretrained(pretrain_path)\n",
    "model=MT5ForConditionalGeneration.from_pretrained(pretrain_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32601, 1024)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(32601, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(32601, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32601, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32601, 1024)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(32601, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(32601, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (17): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (18): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (20): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (21): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (23): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32601, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/model-epoch=01-val_loss=0.4691-val_rougeL_fmeasure=0.779-v1.ckpt/pytorch_model.bin\")\n",
    "# w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/model-epoch=01-val_loss=0.3592-val_rougeL_fmeasure=0.739.ckpt/pytorch_model.bin\")\n",
    "# w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/model-epoch=02-val_loss=0.4445-val_rougeL_fmeasure=0.781.ckpt/pytorch_model.bin\")\n",
    "# w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/model-epoch=01-val_loss=0.7901-val_rougeL_fmeasure=0.649.ckpt/model\")\n",
    "w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/last.ckpt/model\")\n",
    "# w=torch.load('/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/model-epoch=01-val_loss=0.8267-val_rougeL_fmeasure=0.583.ckpt/model')\n",
    "# w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/ckpt/model-epoch=01-val_loss=0.5065-val_rougeL_fmeasure=0.707.ckpt/model.bin\")\n",
    "w2=torch.load('/home/wangran108/code/question-answering/output_dir/pytorch_model.bin')\n",
    "# w2={}\n",
    "# for each in w.keys():\n",
    "#     w2[each[13:]]=w[each]\n",
    "# w2['encoder.embed_tokens.weight']=w['module.model.shared.weight']\n",
    "# w2['decoder.embed_tokens.weight']=w['module.model.shared.weight']\n",
    "\n",
    "# model.load_state_dict(w2)\n",
    "model.to('cuda:0')\n",
    "# w=torch.load(\"/home/wangran108/code/Fengshenbang-LM/fengshen/examples/qa_t5/model-epoch=01-val_loss=0.3905-val_rougeL_fmeasure=0.764.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "file = open(\"/home/wangran108/code/machine_quality_check/case_sample.json\", 'r', encoding='utf-8')\n",
    "# file = open(\"/home/wangran108/code/machine_quality_check/sample5.json\", 'r', encoding='utf-8')\n",
    "\n",
    "# /home/wangran108/code/machine_quality_check/sample.json\n",
    "# example_data2.json\n",
    "all = []\n",
    "for line in file.readlines():\n",
    "    dic = json.loads(line)\n",
    "    all.append(dic)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'客服:您好，这里是京东物流总部请问您请问您是朵西鞋业的吴总吗？顾客:哎对对。客服:您好，无论给您来电时了解一下，就您这边有没有发快递呀？发物流这方面的需求呀。喂你好。顾客:嗯有啊。客服:嗯，咱一般是由什么产品是由这个鞋子吗？您好女士啊。您好，嗯，吴总你好能听见吗？顾客:哎，你好，我说话能听能听见吗？客服:嗯，这位可以了，刚刚好像信号有点乱，就是咱们一般是由这个鞋子吗？顾客:哎，我们主要做的，就是鞋子之类的。',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'context': '顾客:一天发两百多单。客服:一天二百多个200乘以3那一个月差不多，就是5000到1万件儿之间是吧？嗯，那咱这个货品的价值一般，就是在呃50到1000呢，包括吗？顾客:嗯差不多一个月有个六七千件吧。客服:哦，那我就给您登记的50到一千，您看可以吗？顾客:呃，没有我们这因为我一直都价值，整体算下来的话，应该在一千左右。',\n",
    "#    'question': '顾客货物价值是多少',\n",
    "#    'answer': [''],\n",
    "#    'idx': 0,\n",
    "#    'ans_span': []},\n",
    "#      {'context': '顾客:一天发两百多单。客服:一天二百多个200乘以3那一个月差不多，就是5000到1万件儿之间是吧？嗯，那咱这个货品的价值一般，就是在呃50到1000呢，包括吗？顾客:嗯有的,差不多一个月有个六七千件吧。客服:哦，那我就给您登记的50到一千，您看可以吗？顾客:呃，没有我们这因为我一直都价值，整体算下来的话，应该在一千左右。',\n",
    "#    'question': '顾客一个月发单件量是多少，发件次数是多少',\n",
    "#    'answer': [''],\n",
    "#    'idx': 0,\n",
    "#    'ans_span': []},\n",
    "all=[\n",
    "     {'context': '客服:您好，这里是京东物流总部请问您请问您是朵西鞋业的吴总吗？顾客:哎对对。客服:您好，无论给您来电时了解一下，就您这边有没有发快递呀？发物流这方面的需求呀。喂你好。顾客:嗯有啊。客服:嗯，咱一般是由什么产品是由这个鞋子吗？您好女士啊。您好，嗯，吴总你好能听见吗？顾客:哎，你好，我说话能听能听见吗？客服:嗯，这位可以了，刚刚好像信号有点乱，就是咱们一般是由这个鞋子吗？顾客:哎，我们主要做的，就是鞋子之类的。',\n",
    "   'question': '顾客有没有发快递的需求，有没有合作意向',\n",
    "   'answer': [''],\n",
    "   'idx': 0,\n",
    "   'ans_span': []},\n",
    "      {'context': '客服:您好，这里是京东物流总部请问您请问您是朵西鞋业的吴总吗？顾客:哎对对。客服:您好，无论给您来电时了解一下，就您这边有没有发快递呀？发物流这方面的需求呀。喂你好。顾客:暂时不需要。客服:嗯，咱一般是由什么产品是由这个鞋子吗？您好女士啊。您好，嗯，吴总你好能听见吗？顾客:哎，你好，我说话能听能听见吗？客服:嗯，这位可以了，刚刚好像信号有点乱，就是咱们一般是由这个鞋子吗？顾客:哎，我们主要做的，就是鞋子之类的。',\n",
    "   'question': '顾客有没有发快递的需求，有没有合作意向',\n",
    "   'answer': [''],\n",
    "   'idx': 0,\n",
    "   'ans_span': []},\n",
    "       {'context': '客服:您好，这里是京东物流总部请问您请问您是朵西鞋业的吴总吗？顾客:哎对对。客服:您好，无论给您来电时了解一下，考虑跟我们京东合作吗。喂你好。顾客:我发的少。客服:嗯，咱一般是由什么产品是由这个鞋子吗？您好女士啊。您好，嗯，吴总你好能听见吗？顾客:哎，你好，我说话能听能听见吗？客服:嗯，这位可以了，刚刚好像信号有点乱，就是咱们一般是由这个鞋子吗？顾客:哎，我们主要做的，就是鞋子之类的。',\n",
    "   'question': '顾客有没有发快递的需求，有没有合作意向',\n",
    "   'answer': [''],\n",
    "   'idx': 0,\n",
    "   'ans_span': []}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized=[]\n",
    "i1=0\n",
    "i2=100\n",
    "for sample in all[i1:i2]:\n",
    "    plain_text = (\n",
    "                \"question:\"\n",
    "                + sample[\"question\"]\n",
    "                + \"context:\"\n",
    "                + sample[\"context\"][: max_knowledge_length]\n",
    "            )\n",
    "    l_text = len(plain_text)\n",
    "\n",
    "    ctx_len = max_seq_length - l_text - 1\n",
    "    if ctx_len > 0 and \"history\" in sample:\n",
    "        context = \"[SEP]\".join(sample[\"history\"])\n",
    "        plain_text += \"context:\" + context\n",
    "\n",
    "    res_prefix = tokenizer.encode(\"answer:\", add_special_tokens=False)\n",
    "    # res_prefix.tolist()\n",
    "    l_rp = len(res_prefix)\n",
    "\n",
    "    tokenized = tokenizer.encode(\n",
    "        plain_text,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length - 2 - l_rp,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    # tokenized.tolist()\n",
    "    tokenized += res_prefix\n",
    "    # add maskid\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "    tokenized.append(mask_id)\n",
    "    tokenized.append(config.eos_token_id)\n",
    "    all_tokenized.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=torch.tensor(all_tokenized).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  259, 12877,   267,  ...,   267, 32595,     1],\n",
       "        [  259, 12877,   267,  ...,   267, 32595,     1],\n",
       "        [  259, 12877,   267,  ...,   267, 32595,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids=input_ids,labels=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31.2932, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '客服:哦，最多的时候一个月也能发到四五次差不多。顾客:对对对对对。客服:呃，因为您这个文件啊跟这个发票它这个产品价值不是太好估计嘛，然后我这边先帮您登记了50到100元您看行吗？顾客:行行行。', 'question': '顾客一个月发单件量是多少，发件次数是多少', 'answer': ['四五次'], 'idx': 0, 'ans_span': []}\n",
      "四五次\n",
      "{'context': '客服:嗯。顾客:一个月内不整了，一天两百多块。客服:一天二百多个200乘以3那一个月差不多，就是5000到1万件儿之间是吧？嗯，那咱这个货品的价值一般，就是在呃50到1000呢，包括吗？顾客:嗯差不多。客服:哦，那我就给您登记的50兆一千，您看可以吗？顾客:呃，没有我们这因为我一直都价值，整体算下来的话。那那瓶的话，应该在一千左右。', 'question': '顾客一个月发单件量是多少，发件次数是多少', 'answer': ['5000到1万件'], 'idx': 0, 'ans_span': []}\n",
      "5000到1万件\n",
      "{'context': '客服:还还是普通就可以。顾客:哦，最好快一点。客服:嗯，好的，我会帮您备注一下了。呃那咱这个特殊我特殊物流需求有吗？比如说仓储完快运呢？早上服务还有冷链产品这个记港澳台国际。顾客:嗯。呃，有冷运由冷运需求。客服:嗯，好的好的先生，那咱平时之前的话就是邮寄快递的话一般都用的什么用什么快递和物流啊。顾客:顺丰和这个和韵达。哦顺丰和圆通。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷运'], 'idx': 17526, 'ans_span': []}\n",
      "冷运\n",
      "{'context': '客服:嗯。顾客:你把那个玩意儿，晚上收货了，快递都是晚上收货，最迟也就是。客服:行了行。好。好京东在时效和服务商也是顶尖的，这个您放心特殊的物流有吗？比如说寄往港澳台国际的方面的冷链呢？顾客:第三天吧。嗯，没有了，没有了，没有了百分之八九十。百分之八九十都是都是这样的户。客服:哦行，冷链还需要吗？顾客:嗯。客服:嗯。顾客:冷链其实不太需要。客服:因为当天就表示时效比较快的，嗯，行。顾客:金蝶放两瓶，哦，明白。客服:嗯行，另外就是再帮您核实一下，就是那个地址哈是苏州哪个区呢？顾客:振兴区。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 18606, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:哦行，好的，我这边了解了，那您对这个送货时效有没有特殊需求呢？比如说次日达两日达的一般都是三天吗？顾客:嗯，对，一般都三天中奖的吗？客服:是的。嗯，好的，那咱们这个配件的话有没有去港澳台国际就是这些特殊的一个服务需求呢？顾客:偶尔也有。客服:哦哦好的。现在使用的是顺丰吗？女士？顾客:嗯。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 18726, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:哦行，好的，我这边了解了，那您对这个送货时效有没有特殊需求呢？比如说次日达两日达的一般都是三天吗？顾客:嗯，对，一般都三天中奖的吗？客服:是的。嗯，好的，那咱们这个配件的话有没有去港澳台国际就是这些特殊的一个服务需求呢？顾客:很少有。客服:哦哦好的。现在使用的是顺丰吗？女士？顾客:嗯。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['很少'], 'idx': 18726, 'ans_span': []}\n",
      "很少\n",
      "{'context': '客服:哦行，好的，我这边了解了，那您对这个送货时效有没有特殊需求呢？比如说次日达两日达的一般都是三天吗？顾客:嗯，对，一般都三天中奖的吗？客服:是的。嗯，好的，那咱们这个配件的话有没有去港澳台国际就是这些特殊的一个服务需求呢？顾客:比较少。客服:哦哦好的。现在使用的是顺丰吗？女士？顾客:嗯。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['比较少'], 'idx': 18726, 'ans_span': []}\n",
      "比较少\n",
      "{'context': '客服:哦行，好的，我这边了解了，那您对这个送货时效有没有特殊需求呢？比如说次日达两日达的一般都是三天吗？顾客:嗯，对，一般都三天中奖的吗？客服:是的。嗯，好的，那咱们这个配件的话有没有去港澳台国际就是这些特殊的一个服务需求呢？顾客:有时候会。客服:哦哦好的。现在使用的是顺丰吗？女士？顾客:嗯。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 18726, 'ans_span': []}\n",
      "有时候会\n",
      "{'context': '客服:嗯，好的好的，嗯，这个送货时效的话就是正常时效就行是吧。顾客:对。客服:哦行，港澳台国际呀，快运等等这些都没有。顾客:嗯比较少。客服:哦行，那我这边先给咱们登记一个无特殊需求，咱们现在发的是顺丰的吗？顾客:嗯，顺丰也发，然后平常的那个当地的物流也发。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['比较少'], 'idx': 19166, 'ans_span': []}\n",
      "比较少\n",
      "{'context': '客服:嗯，好明白时效要求高吗？就是省内次日达省外三天的时效可以吗？顾客:那就按照咱们京东快递的标准。客服:嗯，好的明白特殊物流需求有吗？比如说g港澳台国外呀，或者仓储快运啊，整出啊这种。顾客:这个暂时还没。客服:行，您现在是用顺丰只多是吗？顾客:现在什么都有。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['暂时还没'], 'idx': 19436, 'ans_span': []}\n",
      "还没有\n",
      "{'context': '客服:嗯，那就给您登记三十无特殊要求正常时效可以吗？顾客:嗯，行，行行，可以。客服:嗯，您有特殊的物流需求嘛，仓储快运整车市医疗产品邮寄冷链产品港澳台国际有需求吗？顾客:运货市经常有我们经常用那个货拉拉。客服:哦就是快运吗？还是整车服务。顾客:嗯。有整车快运。客服:行，那这两个给您标记上，嗯，那您现在用的是吗？顾客:你们从。稍等别着急啊。客服:嗯好的。顾客:你们那个你们那个有有从哪儿吗？假如说我从山东我用17米长的车能否运过来嘛，应该是有吗？客服:嗯，这个可以咨询一下销售经理正常是我们这边什么车型中油的，我先把快要和整车服务给您登记的。销售经理会回答您的问题好的，您现在用的是哪一个物流或者快递呀。顾客:嗯。行行。不一定啊，你这这首先找嘛，找物流公司呢。客服:嗯，您说一下，我给您备注一下，就是都用过什么呢？顾客:就正常的物流公司当地找，你说从江苏发货，就从江苏当地找物流公司了。客服:哦，就是那种私人的是吗？顾客:发内有金属的。嗯，就建筑材料吗？客服:噢，就是私人的物流是吧，用的是就是个人的那种。顾客:对对对对。客服:哦哦好的，那就是那他给您大概多少价位啊，多少折您了解吗？顾客:多少折。客服:嗯。顾客:嗯，折那不一定谈呢？客服:哎，好的没关系的，那您有朋友需要申请这个优惠吗？可以推荐给京东一个电话吗？顾客:嗯，咱俩先谈着再说吧？行吗？客服:嗯，行好的，那已经帮您申请了，您注意接听电话就可以了，好吧。顾客:嗯，好嘞，好嘞，嗯，嗯。客服:嗯，好的，祝您生活愉快，再见。顾', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车快运'], 'idx': 19606, 'ans_span': []}\n",
      "整车快运\n",
      "{'context': '客服:好，那就正常的先给您备注正常的没有特殊要求。顾客:对。客服:呃，你有没有特殊的物流需求？比如说整车的服务或者期从港澳台的。顾客:对对对。嗯，但是香港呢，会有我们就是整车，但是就看你们这个就是说这个整车的结果我们主要是看对比价格吧，对吧，咱们有啥算一个做生意的人。客服:嗯行。哦行，整车的也有港澳台的也有是吧。顾客:对都有。客服:嗯，比较少，哦，您现在用的什么快递呢？顾客:嗯，用的比较呃，比较早，现在京东也给你们用着呢，但是没签协议京东呢也在做，但没签协议可能。因为啥呢，可能价格没谈妥。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 19756, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:嗯。国际旅行社的负责人，哦好的，那您这对这个送货的时效有什么要求吗？比如当日达次日达或者就是没有什么特殊要求。顾客:嗯，有需要会的时候也有正常都行，嗯。客服:哦哦，那我跟您后边备注一下，看有的时候需要根据客户需求好可以吧。那这边的话，您平时您有这种寄送港澳台国际的业务嘛就。顾客:嗯。客服:您这个系统港澳台国际的这种业务吗？或者是这些文件之类的。顾客:嗯。少。客服:少有什么那后边儿我给您备注一下无特殊需求您平时的话记这个快件的话是用顺丰还是三通一达还是。居住吧，是他给我们德邦呀，还是由我们京东呀。顾客:京东对两年。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['少'], 'idx': 19836, 'ans_span': []}\n",
      "少\n",
      "{'context': '客服:嗯。国际旅行社的负责人，哦好的，那您这对这个送货的时效有什么要求吗？比如当日达次日达或者就是没有什么特殊要求。顾客:嗯，有需要会的时候也有正常都行，嗯。客服:哦哦，那我跟您后边备注一下，看有的时候需要根据客户需求好可以吧。那这边的话，您平时您有这种寄送港澳台国际的业务嘛就。顾客:嗯。客服:您这个系统港澳台国际的这种业务吗？或者是这些文件之类的。顾客:嗯。少。客服:少有什么那后边儿我给您备注一下无特殊需求您平时的话记这个快件的话是用顺丰还是三通一达还是。居住吧，是他给我们德邦呀，还是由我们京东呀。顾客:京东对两年。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['少'], 'idx': 19836, 'ans_span': []}\n",
      "有\n",
      "{'context': '客服:嗯。国际旅行社的负责人，哦好的，那您这对这个送货的时效有什么要求吗？比如当日达次日达或者就是没有什么特殊要求。顾客:嗯，有需要会的时候也有正常都行，嗯。客服:哦哦，那我跟您后边备注一下，看有的时候需要根据客户需求好可以吧。那这边的话，您平时您有这种寄送港澳台国际的业务嘛就。顾客:嗯。客服:您这个系统港澳台国际的这种业务吗？或者是这些文件之类的。顾客:嗯。很少。客服:少有什么那后边儿我给您备注一下无特殊需求您平时的话记这个快件的话是用顺丰还是三通一达还是。居住吧，是他给我们德邦呀，还是由我们京东呀。顾客:京东对两年。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['很少'], 'idx': 19836, 'ans_span': []}\n",
      "很少\n",
      "{'context': '客服:嗯，先生呃，好的先生就是咱个送货时效咱这边是怎么要求的？咱这边政策是两三天送到也可以当天或者第二天送的。顾客:一般都是第二天吧。客服:第二天是吧，好嘞好嘞，然后的话现在咱们咱们京东才能给您做一些增值服务先给您提供这个仓储就是全国的这个京东仓库都可以为您或者快运邮寄港澳台冷链冷链渠道这种能用的吗？顾客:仓储可以呀，仓储费用高吗？客服:仓储。呃仓储的话费用咱们挺便宜的可以让那边到时候呃这两天联系您说给您报个价你看你看下合不合适划不划算？哎好嘞好嘞，谢谢。先生就是您这边主要与顺丰快递是吗？顾客:好吧嗯。你的仓储需要我的人过去吗？不需要吧？', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 20016, 'ans_span': []}\n",
      "不需要\n",
      "{'context': '客服:呃，您现在用的这一个就是快递是哪一个快递呀，先生。顾客:我们现在多了，就是有的是顺丰大部分顺丰。客服:哦，打不完使用的这个审核您那您这边的话有这个仓储需求邮寄港澳台这一块的需求吗？顾客:嗯偶尔有。客服:啊，有这个邮寄港澳台的。顾客:我说。不多。我刚查完以后，我们手工设备，从台湾寄过来比较多。客服:哦不多，哦，明白明白。哦，寄过来的多，往外邮的比较少，对吧？顾客:是谁在台湾生产的东西的。客服:哦，明白明白好的后期的话。就让我们去约的销售经理啊，就联系您这个1578的这个手机号可以吗？顾客:嗯，嗯好，谢谢。客服:哦好让啊，到时候让他加您的微信给您谈具体的就是表扣哦好好，嗯，那这个地址北京市延庆区这块儿对吗？顾客:好，我知道了。对，是我我在广东佛山。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不多'], 'idx': 21476, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:噢，那我先暂时帮咱勾选在次日的，您看可以吗？顾客:嗯行。客服:好的那特殊物流有需求嘛，比如仓库快运整车服务或者说g港澳台国际这些。顾客:嗯有整车。客服:噢好的。呃，咱现在是在用哪些快递呀，或者是顺丰德邦这些吗？顾客:哎对呀。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 21796, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:呃，资料文件文件类，那咱对这个时效的话，咱们京东时效是省内次日达全国的话三十左右，这个时效可以满足吗？顾客:可以。客服:行，您这边儿有有需要寄到国际港澳台这些特殊的需求吗？顾客:也有去寄到香港有。客服:香港有是吧，行，那我帮您登记上，您现在的话一般都是用什么快递呀。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 22386, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:两公斤。哦，十公斤以下那行我给您登记1到20公斤之内，然后您这边对那个时效要求有没有？嗯要求标次日达当当日达两字哒，三日或者无要求。顾客:时效的话肯定是至少要三天以内吧，太长的话，就没有意义了。客服:哦。哦，那行那您对这物流有没有特殊需求别让仓储整车？然后这个呃冷链这块儿。呃这个。顾客:我们主要是快递，或者是物流，或者是快运。客服:哦行，那我给您备注快运快递物流这三个然后您这边过往合作的话有没有优惠？然后顺丰圆通中通这一块有没有或者京东有没有合作的？顾客:都有，我们都有合作。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['快运'], 'idx': 22786, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:那像我一下呢。好的好的好的，您这边送货时效上有特殊的要求吗？比如说是当日达次日达隔日达还是三日或者无特殊要求。顾客:嗯，本市的话需要当日，然后外服的话，这个及外地的话下次。客服:嗯。嗯。好的，您需要特殊的物流需求吗？就是说仓储快运整车服务及港澳台国际这些？顾客:嗯。港澳台偶尔会发，但是一年用不到一两次。客服:呃，好的，我先帮您登记上，如果您后续要用的话也可以用啊，如果不用也没关系，然后您这边目前合作的是顺丰和百世是吗？顾客:嗯，就是顺丰还有韵达。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 23716, 'ans_span': []}\n",
      "偶尔会发\n",
      "{'context': '客服:那像我一下呢。好的好的好的，您这边送货时效上有特殊的要求吗？比如说是当日达次日达隔日达还是三日或者无特殊要求。顾客:嗯，本市的话需要当日，然后外服的话，这个及外地的话下次。客服:嗯。嗯。好的，您需要特殊的物流需求吗？就是说仓储快运整车服务及港澳台国际这些？顾客:嗯。港澳台偶尔会发，但是一年用不到一两次。客服:呃，好的，我先帮您登记上，如果您后续要用的话也可以用啊，如果不用也没关系，然后您这边目前合作的是顺丰和百世是吗？顾客:嗯，就是顺丰还有韵达。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 23716, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:那像我一下呢。好的好的好的，您这边送货时效上有特殊的要求吗？比如说是当日达次日达隔日达还是三日或者无特殊要求。顾客:嗯，本市的话需要当日，然后外服的话，这个及外地的话下次。客服:嗯。嗯。好的，您需要特殊的物流需求吗？就是说仓储快运整车服务及港澳台国际这些？顾客:嗯。港澳台很少，但是一年用不到一两次。客服:呃，好的，我先帮您登记上，如果您后续要用的话也可以用啊，如果不用也没关系，然后您这边目前合作的是顺丰和百世是吗？顾客:嗯，就是顺丰还有韵达。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['很少'], 'idx': 23716, 'ans_span': []}\n",
      "很少\n",
      "{'context': '客服:那像我一下呢。好的好的好的，您这边送货时效上有特殊的要求吗？比如说是当日达次日达隔日达还是三日或者无特殊要求。顾客:嗯，本市的话需要当日，然后外服的话，这个及外地的话下次。客服:嗯。嗯。好的，您需要特殊的物流需求吗？就是说仓储快运整车服务及港澳台国际这些？顾客:嗯。港澳台经常发的。客服:呃，好的，我先帮您登记上，如果您后续要用的话也可以用啊，如果不用也没关系，然后您这边目前合作的是顺丰和百世是吗？顾客:嗯，就是顺丰还有韵达。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 23716, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:呃，走的是物流大批量那也挺多的，那我这边暂时给您勾选1到20公斤，您看可以吗？顾客:对。哦行。客服:嗯，好的，嗯，我们这边的话呢，还提供那个宽裕呀，仓储需求呀，去港澳台国际您看有这个特殊需求吗？顾客:呃仓储和什么。客服:仓储啊快运呀整车服务呀？您看有吗？这个需求。顾客:整车服务的话，我们有固定。客服:有固定的是吧，那我这边就给您勾选上，后期您要是觉得可以的话也可以跟我们这个合作，您看可以吧。顾客:嗯嗯行。客服:好的，您对我们这个送货时效要有要求吗？比如说当日达次日达三日或者无测序要求有吗？顾客:嗯，我们要不然有这种特殊要求的。我们现在用的是跨越呃跨越提示是可以选什么当日次日什么隔日陆运什么的，呃就是你能让我去选就行。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 24076, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:呃，您现在用的这一个就是快递是哪一个快递呀，先生。顾客:我们现在多了，就是有的是顺丰大部分顺丰。客服:哦，大部分人使用的这个审核您，那您这边的话有这个仓储需求邮寄港澳台这一块的需求吗？顾客:嗯偶尔有。客服:啊，有这个邮寄港澳台的。顾客:我说。不多。我刚查完以后，我们手工设备，从台湾寄过来比较多。客服:哦不多，哦，明白明白。哦，寄过来的多，往外邮的比较少，对吧？顾客:是谁在台湾生产的东西的。客服:哦，明白明白好的后期的话。就让我们去约的销售经理啊，就联系您这个1578的这个手机号可以吗？顾客:嗯，嗯好，谢谢。客服:哦好让啊，到时候让他加您的微信给您谈具体的就是表扣哦好好，嗯，那这个地址北京市延庆区这块儿对吗？顾客:好，我知道了。对，是我，我在广东佛山。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不多'], 'idx': 24406, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:好的，那给您选这个正常的三天，您看可以吗？顾客:正常三天。客服:就是送货的时效，您这个特殊的物流需求有要求吗？比如昌储蓄球袜快运或者是整车服务这种的。顾客:嗯。有，就是比较着急的呀。客服:着急的快运明显一个，您看可以吧。顾客:嗯。客服:好的，您平时有其其他的快递吗，比如顺丰啊，或者是三通一达一类的。顾客:我基本上都是用顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['快运'], 'idx': 24766, 'ans_span': []}\n",
      "快运\n",
      "{'context': '客服:好，请您对送货时效有要求吗？还是正常时效就可以了？顾客:嗯，对，正常就可以。客服:行，您有餐厨快运整车服务邮寄港澳台这些需求吗？顾客:嗯，这个比较少。客服:嗯，行，您之前用的都是哪些快递呀。顾客:嗯，几乎用的都是顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['比较少'], 'idx': 24856, 'ans_span': []}\n",
      "比较少\n",
      "{'context': '客服:嗯，行送货时效您有要求吗？比如说当日达次日达或者正常时效就可以。顾客:无所谓正常了呀。客服:嗯，好，那我今晚港澳台国际的这些需求吗？顾客:很少。客服:嗯行，那用顺丰的话，他给您的乘客大概有多少有折扣吗？有个九折。顾客:幺儿幺有没老，老顺丰客户联联系，我就行了。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['很少'], 'idx': 24946, 'ans_span': []}\n",
      "很少\n",
      "{'context': '客服:好的，嗯这个送货时效要求吗，比如说是当天到或者是三天左右。顾客:三天左右。客服:好嘞，嗯，这个有没有特殊的物流需求呢？比如说仓储快运啊，这个有要求吗？顾客:哦，没有，就是正常的时间就可以。客服:嗯，行，嗯，之前就是发过什么快递呀。顾客:我们之前就是圆通跟申通。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['没有'], 'idx': 25126, 'ans_span': []}\n",
      "正常时间\n",
      "{'context': '客服:嗯，发的您是往哪里发呀？先生。顾客:美国。客服:发美国应该是可以的，我这边有一个特殊的物流需求是邮寄港澳台国际的，我给您勾选一下，然后需销售经理跟您联系的时候您可以跟他了解一下，如果要是发到美国的话是怎么收费的可以吗？我给您勾选了。顾客:好好。客服:好的那行，您还有其他需要了解的吗？先生。顾客:嗯没有了。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 25716, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:呃，对那个物流呢，物流有需求嘛，比如比如说需要寄到港澳台啊，或者外国哪些地方？顾客:偶尔会有。客服:嗯，好的，呃，偶尔会有是寄到国外还是就是港澳台这些地方呢？顾客:国外。客服:嗯，我我们之前就是目目前用的是哪个快递？或者是物流公司的。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 26176, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:呃，对那个物流呢，物流有需求嘛，比如比如说需要寄到港澳台啊，或者外国哪些地方？顾客:偶尔会发到香港。客服:嗯，好的，呃，偶尔会有是寄到国外还是就是港澳台这些地方呢？顾客:国外。客服:嗯，我我们之前就是目目前用的是哪个快递？或者是物流公司的。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 26176, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:呃，对那个物流呢，物流有需求嘛，比如比如说需要寄到港澳台啊，或者外国哪些地方？顾客:偶尔会发到台湾。客服:嗯，好的，呃，偶尔会有是寄到国外还是就是港澳台这些地方呢？顾客:国外。客服:嗯，我我们之前就是目目前用的是哪个快递？或者是物流公司的。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 26176, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:呃，对那个物流呢，物流有需求嘛，比如比如说需要寄到港澳台啊，或者外国哪些地方？顾客:偶尔会发到国外。客服:嗯，好的，呃，偶尔会有是寄到国外还是就是港澳台这些地方呢？顾客:国外。客服:嗯，我我们之前就是目目前用的是哪个快递？或者是物流公司的。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 26176, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:呃，对那个物流呢，物流有需求嘛，比如比如说需要寄到港澳台啊，或者外国哪些地方？顾客:很少会有。客服:嗯，好的，呃，偶尔会有是寄到国外还是就是港澳台这些地方呢？顾客:国外。客服:嗯，我我们之前就是目目前用的是哪个快递？或者是物流公司的。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['很少'], 'idx': 26176, 'ans_span': []}\n",
      "很少\n",
      "{'context': '客服:哦没有。啊行，就如果说有特殊的可以备注是吧？顾客:对对对。客服:嗯，那我们现在的话需要寄国外吗？就去港澳台国际还说都是寄到国内呀？顾客:都是，国内。客服:都是国内的啊。行，那我们的税，但这个的话我们的呃电子还是在我们的，北京市顺义区，就是物流基地这边吗？顾客:嗯。不是不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['国内'], 'idx': 26276, 'ans_span': []}\n",
      "国内\n",
      "{'context': '客服:哦没有。啊行，就如果说有特殊的可以备注是吧？顾客:对对对。客服:嗯，那我们现在的话需要寄国外吗？就去港澳台国际还说都是寄到国内呀？顾客:都是，国内。客服:都是国内的啊。行，那我们的税，但这个的话我们的呃电子还是在我们的，北京市顺义区，就是物流基地这边吗？顾客:嗯。不是不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['国内'], 'idx': 26276, 'ans_span': []}\n",
      "国内\n",
      "{'context': '客服:哦没有。啊行，就如果说有特殊的可以备注是吧？顾客:对对对。客服:嗯，那我们现在的话需要寄国外吗？就去港澳台国际还说都是寄到国外呀？顾客:都是，国外。客服:都是国外的啊。行，那我们的税，但这个的话我们的呃电子还是在我们的，北京市顺义区，就是物流基地这边吗？顾客:嗯。不是不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 26276, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:哦没有。啊行，就如果说有特殊的可以备注是吧？顾客:对对对。客服:嗯，那我们现在的话需要寄国外吗？就去港澳台国际还说都是寄到国外呀？顾客:都是，国外。客服:都是国外的啊。行，那我们的税，但这个的话我们的呃电子还是在我们的，北京市顺义区，就是物流基地这边吗？顾客:嗯。不是不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 26276, 'ans_span': []}\n",
      "外国\n",
      "{'context': '客服:哦好的那我。顾客:然后百世快运的话是8毛1公斤，嗯，一般就是到200公斤就可以用八毛钱的不到200公斤的时候是1块1。客服:嗯，好的，我这边帮您记录一下，嗯那咱们还需要一些其他的特殊物流需求吗？比如说仓储快运整车服务冷链和邮寄港澳台的业务。顾客:啊。需要寄港澳台不需要冷链。客服:嗯好的女士。嗯，那是这样，我现在跟您核对一下，咱们的一个发货地址是否是北京市顺义区军营街16号院10号楼呢？女士？顾客:不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 27516, 'ans_span': []}\n",
      "不需要冷链\n",
      "{'context': '客服:哦好的那我。顾客:然后百世快运的话是8毛1公斤，嗯，一般就是到200公斤就可以用八毛钱的不到200公斤的时候是1块1。客服:嗯，好的，我这边帮您记录一下，嗯那咱们还需要一些其他的特殊物流需求吗？比如说仓储快运整车服务冷链和邮寄港澳台的业务。顾客:啊。需要冷链不需要寄港澳台。客服:嗯好的女士。嗯，那是这样，我现在跟您核对一下，咱们的一个发货地址是否是北京市顺义区军营街16号院10号楼呢？女士？顾客:不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 27516, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:应该不会超过20公斤吧？咱们发一次货正常的不会是吧，就是说时间有要求吗？先生全国，我们京东是正常是三天左右，您看这个时效可以吗？顾客:呃不超过。嗯。哦。客服:全国。嗯，如果您如果您是同城的话，次日就可以到达，您是发哪边多一点？都有是吧，那我就按正常时效给您记录吧，好吧，三点左右您需要仓储马俊先生需要仓储吗，包括港澳台寄过去需要吗？顾客:好吧，好的嗯。嗯。仓储，你们那是什么仓储？嗯。客服:呃，仓储就是可以帮您待发货就是您做好的比如说广告什么的可以放仓储里面的，如果您需要发货直接给您发出去，但是我不知道您那个是不是定，如果属于定制的话，可能就用不着。顾客:嗯，是我这儿材质。客服:哦，那可能就仓储可能用不着，那我给您暂时就无需您目前的话是跟哪哪个那个快递在合作先生您这边？顾客:龚龚。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['用不着'], 'idx': 27716, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:应该不会超过20公斤吧？咱们发一次货正常的不会是吧，就是说时间有要求吗？先生全国，我们京东是正常是三天左右，您看这个时效可以吗？顾客:呃不超过。嗯。哦。客服:全国。嗯，如果您如果您是同城的话，次日就可以到达，您是发哪边多一点？都有是吧，那我就按正常时效给您记录吧，好吧，三点左右您需要仓储马俊先生需要仓储吗，包括港澳台寄过去需要吗？顾客:好吧，好的嗯。嗯。仓储，你们那是什么仓储？嗯。客服:呃，仓储就是可以帮您待发货就是您做好的比如说广告什么的可以放仓储里面的，如果您需要发货直接给您发出去，但是我不知道您那个是不是定，如果属于定制的话，可能就不需要。顾客:嗯，是我这儿材质。客服:哦，那可能就仓储可能不需要，那我给您暂时就无需您目前的话是跟哪哪个那个快递在合作先生您这边？顾客:龚龚。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不需要'], 'idx': 27716, 'ans_span': []}\n",
      "不需要\n",
      "{'context': '客服:应该不会超过20公斤吧？咱们发一次货正常的不会是吧，就是说时间有要求吗？先生全国，我们京东是正常是三天左右，您看这个时效可以吗？顾客:呃不超过。嗯。哦。客服:全国。嗯，如果您如果您是同城的话，次日就可以到达，您是发哪边多一点？都有是吧，那我就按正常时效给您记录吧，好吧，三点左右您需要仓储马俊先生需要仓储吗，包括港澳台寄过去需要吗？顾客:好吧，好的嗯。嗯。仓储，你们那是什么仓储？嗯。客服:呃，仓储就是可以帮您待发货就是您做好的比如说广告什么的可以放仓储里面的，如果您需要发货直接给您发出去，但是我不知道您那个是不是定，如果属于定制的话，可能就用不到。顾客:嗯，是我这儿材质。客服:哦，那可能就仓储可能用不到，那我给您暂时就无需您目前的话是跟哪哪个那个快递在合作先生您这边？顾客:龚龚。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['用不到'], 'idx': 27716, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:哦，物流也用发产品。哦，那到时候让我们那我先一文件的方式给咱们简单登记，到时候，我稍后备注，让咱销售给您细讲行吗？顾客:对。对。可以。客服:嗯，行，那咱们就是说有那个国际快递吗，比如说港澳台国际之类的。顾客:国际英国的，行么，国外。客服:国外是吧，我们是有港澳台国际服务的是英国的是吧。顾客:啊。对。客服:那咱们一般公司在合作的是哪些快递呢，比如说三通一达，或者是德邦顺丰。顾客:我们我们都不都，嗯，合作顺丰比较多。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 27896, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:到了好的那这边给您备注100公斤以内，然后您的这个送货时效有要求吗？比如说要呃除了禁地的我们要就是呃，就是正常是要正常的走件速度还是说呃，就是我们每单要啊，比如说要当日达呀，或者是说48小时必须到达这些有要求吗？送货时效。顾客:这个看客户需求。客服:行好的，那就是暂时没有特殊的这个需求勘测客户的要求了您这边有特殊的物流需求吗？比如说有的时候您需要夏天需要发视频的时候冷链服务呀，还有就是说比如寄国外港澳台这些特殊的需求，如果有的话，这边给您备注上。顾客:冷链的，有时候会可能会有吧？客服:有的好的，我这边先给您备注有的东西需要冷链，嗯，那像您有这么大的发电量呃，您这边经常合作的这个物流是哪哪个比较多呀？比如说德邦安能还是说您当地的物流多一点。顾客:嗯，我们基本上都有顺丰啊，申通啊，韵达还是啊。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 28116, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:好的好的，那女士想问一下，您现在都用了哪些快递呢，顺丰中通韵达之类的吗？顾客:嗯，都有云南，然后申通圆通？嗯。客服:呃，好的好的好的。他们有没有给您一个折扣价格呢？或者说是优惠券。或者是哒。好的，嗯，那您是应该没有什么特殊的物流需求吧，就例如仓储快运整车服务之类的应该都没有吧？顾客:有啊。仓储你们做仓储吗？客服:哦，我们有我们京东是有删除的，就是您把您的货放在我们那个京东的仓库里，然后有时间就是发货。这个是最快的一个。顾客:噢，那仓储呢，还是怎么计费的？客服:嗯，这个稍后我会让相关人员给您一个具体的报价我就先先先给您记录一下您这个有有有可能需要这个仓储这个仓储需求您看可以吧，好的，您对于送货的时效有什么要求吗？就例如当日达世达或者是生日无特殊需求。顾客:ok。好的好的。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 17396, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:哎，好的特殊物流呢，仓储快运整车国际港澳台这边有需求吗？顾客:有。客服:先是发往国外吗？还是需要用到快运仓储这边？顾客:嗯，用户的国外吧。客服:嗯，好的，您这个一般都邮寄什么物品呀？顾客:电子产品。客服:电子产品嗯，每个月发件量的话，大概有多少呢，100到300还是300到1000左右呀。顾客:呃，没那么多比较少。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 16966, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:噢，那您这边对于这个送货时效有要求吗？就是当日达次日达正常时效。顾客:呃正常就行。客服:啊好的您这边有特殊的物流需求吗？像仓储快运冷链港澳台过去有这方面的需求吗？顾客:呃。暂时没有。客服:没有是吧，后期可能会有用的对吧，啊，那到时候帮您备注一下。顾客:对对，后期可能会用客服:啊好的，那咱这边平长使用的快递都有哪些呢？顾客:嗯。顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['暂时没有'], 'idx': 16486, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:哦对。对对，对我明白的先生。嗯那先生给他选择呃，次日达，然后最迟不能超过三天这个范围好吗？顾客:嗯，嗯，可以可以，嗯。客服:嗯，好的，嗯那先生您这里的话还有特殊的物流需求吗？咱们这边还可以提供仓储快运整车还有寄送港澳台的服务有需要吗？先生？顾客:呃，这个有需要可以可以有需要，嗯。客服:呃，像您需要哪一项服务呢，是需要快运还是等车。还是冷链。顾客:冷链。客服:冷链好的好的，嗯，那先生您之前有没有发过其他的快递公司呀？像顺丰德邦中通韵达这些有发过吗？顾客:嗯。嗯。嗯。我发个顺丰啊。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 15676, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:嗯好的。顾客:那生鲜需要快速。客服:嗯，行，您有这个快运的需求吧？就呃，然后还有那个冷链，您需要么就是水果需要冷冻吗？顾客:冷链，我现在跟顺丰这边菏泽市冷链。客服:嗯行。哦行，我我标上嗯，然后您这边顺丰的话有给您打折方能给到多少的折扣呢？顾客:嗯姓方。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 15366, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:哦好的我这边呢，暂时帮你勾选正常时效，您看可以吗，三日。顾客:可以。客服:嗯，好的，咱们对哪一个仓储快运整车港澳台冷链这方面有需求吗？顾客:由我们那个有的时候都是几十几十的都发货。客服:还行吧。哦就是快运呀还是整车呢？帮你勾选整车服务，你看可以吗？顾客:可以。客服:嗯，好的，咱们平时记就是。顾客:我估计你们没有有优势。客服:哈哈哈，咱们先统计一下吧，然后后期的话我们去销售会具体的和您洽谈您常用的物流或者说快递有哪些呢，比如说顺丰三通韵达。常用的是哪个啊？顾客:顺丰其它不用对。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车服务'], 'idx': 15056, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:哦好的，然后您这个时效正常时效就可以是吗？顾客:对，正常时效是可以。客服:嗯，涉及到这个快运吗？因为您这边不是有中货嘛，这个快运和整车服务这边有需要吗？顾客:嗯，整车服务基本上面有，但是如果是快运，呃他有区别吗？客服:嗯。偶尔的优惠是吧？呃，没啥区别，就是问一下您这边有没有这个需要？顾客:哦。客服:这个快运的话，会涉及到吗？顾客:也有设置那个中国一样情况下，那我有时候发动货以后发快运呢，因为快件是不是就到当地不送货上门？客服:嗯。不是就正常的那种快运都是一样的，然后您的公司名称是和胜美涂装有限公司，您是王先生发货地址在通州永乐永乐甸这边是吗？顾客:不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 14316, 'ans_span': []}\n",
      "快运\n",
      "{'context': '客服:嗯。顾客:嗯，对，已基本上就是20公斤以内没有，超过很少的。客服:嗯，行，咱这边有没有特殊的物流需求呢？就是。顾客:发国外的也有。客服:对有是吗？那好我这边给咱钩选上，那咱这边的话之前合作顺丰邮给咱什么优惠吗？顾客:有。嗯，有的它是打折吧，好像是这个具体物流这块。客服:嗯，嗯，可是我看月底还是我明白，嗯。顾客:我没啊。客服:嗯，那咱对好的好的送货的时效有要求吗？因为我们京东现在推出的有这个当日达次日达正常的话，还有这种三天的这种咱这边有这个要求吗？顾客:有有优惠。哦有有。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 14216, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:嗯。顾客:嗯，对，已基本上就是20公斤以内没有，超过很少的。客服:嗯，行，咱这边有没有特殊的物流需求呢？就是。顾客:发国外的也有。客服:对有是吗？那好我这边给咱钩选上，那咱这边的话之前合作顺丰邮给咱什么优惠吗？顾客:有。嗯，有的它是打折吧，好像是这个具体物流这块。客服:嗯，嗯，可是我看月底还是我明白，嗯。顾客:我没啊。客服:嗯，那咱对好的好的送货的时效有要求吗？因为我们京东现在推出的有这个当日达次日达正常的话，还有这种三天的这种咱这边有这个要求吗？顾客:有有优惠。哦有有。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 14216, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:嗯。顾客:嗯，对，已基本上就是20公斤以内没有，超过很少的。客服:嗯，行，咱这边有没有特殊的物流需求呢？就是。顾客:发国外的也有。客服:对有是吗？那好我这边给咱钩选上，那咱这边的话之前合作顺丰邮给咱什么优惠吗？顾客:有。嗯，有的它是打折吧，好像是这个具体物流这块。客服:嗯，嗯，可是我看月底还是我明白，嗯。顾客:我没啊。客服:嗯，那咱对好的好的送货的时效有要求吗？因为我们京东现在推出的有这个当日达次日达正常的话，还有这种三天的这种咱这边有这个要求吗？顾客:有有优惠。哦有有。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 14216, 'ans_span': []}\n",
      "对\n",
      "{'context': '客服:对吧，那那他那他们有没有给您一个折扣价格呢？或者说是给您打了几折优惠。顾客:有有有那个当然好了，长期长期发了，当，然有那个京东差不多德邦的是七折那个三通一达就便宜多了。客服:呃呃呃呃。德邦徐子涵。好的好的好的，呃想问一下您是否有特殊的物流需求呢，就例如仓储快运整车服务这一类的。顾客:没了，没没有啊，有有有一些代收货款而已。客服:好的。呃，好的，呃想问一下您是否，呃您对于送货的时效有什么要求吗？就例如当日达次日达或者是三十无特殊需求。顾客:喂。没有没有没有没有。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['没有'], 'idx': 14096, 'ans_span': []}\n",
      "仓储快运整车\n",
      "{'context': '客服:嗯，好的好的，那我帮您登记了一公斤的100公斤以内呃，您对于送货的时效有什么要求吗？就例如当日达次日达或者是生日无特殊需求。顾客:嗯，3到5天3到5日之前。客服:嗯好。嗯，好的好的好的，嗯，您看您是否有特殊的物流需求就例如仓储快运整车冷链服务之类的。顾客:整车。客服:嗯，整车嗯，好的整车呃，您现在用的都是哪些快递呢？就是顺丰申通中通德邦之类的吗？顾客:喂顺丰申通德邦。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 13916, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:哎，好的，那您有时效要求吗？当日达次日达两周内或者三日达我要要求。顾客:是的。客服:好的，您有特殊需求嘛，仓储快运整数服务医疗产品邮寄龙面上面港澳台国际有需求吗？顾客:这个我们是那个什么走的，这一类是走的物流。客服:噢噢噢，行好的，就是暂时没有需求是吧？那您现在用顺丰他给您有折扣吗？顾客:对。呃有磕碰。包括积分什么的积分月付，对不对啊？', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['没有需求'], 'idx': 13566, 'ans_span': []}\n",
      "没有需求\n",
      "{'context': '客服:哦好的，那您对送货时效有要求吗？比如说当日次日或者说按京东正常的时效。顾客:正常时效就行，我们这边会就不亮。客服:嗯，好的好的，那你会用到那个特殊的物流需求嘛，像拼车整车这类服务。顾客:整车。客服:嗯。顾客:整个有可能会用得着，你如果大设备有可能哦，对那大神能忘了。我问他要四五百公交车，没有。客服:还有四五百公斤的是吗？那我这边都帮您勾选上。那那您这个每个月发电量我就帮您共享到50到100了，因为我看你发到也比较咱后续的话，根据实际情况而定。顾客:嗯，好吧，那是什么店？喂。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 12256, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:好的好的，然后咱这边对一个送货时效有什么特殊要求吗？比如当日达次日达或者是正常就行。顾客:哦。正常就行吧。客服:哎，好嘞，因为咱这个是食品吗？您对那个冷链有特殊的一个物流要求吗？顾客:喂。是的，是的，必须要冷链呀。客服:嗯，好的，咱这边之前使用的是哪个快递呀？顾客:之前也是京东，但是有时候运送方有时候用多高。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 11176, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:呃，沈万是吧，那我就要嗯。顾客:你们没有，你们有你们有空运吗？客服:可以的先生，我们这边有嗯，有那个整车快运快运服务，包括那个港澳台需求仓储，您这边需要吗？顾客:你们仓储仓储包，包括报关这些吗？客服:哦，对对，就是说仓储他是就是帮您待发货嘛，直接从咱们仓储这边发货，那这边我先帮您记录上，后续我这边直接让业务来给您那个就直接沟通就行，您目前就是咱们那个发货地的话是在那个东莞市南城街道这边吗？顾客:嗯。哦。哦。东莞。对对。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 10636, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:呃，沈万是吧，那我就要嗯。顾客:你们没有，你们有你们有空运吗？客服:可以的先生，我们这边有嗯，有那个整车快运快运服务，包括那个港澳台需求仓储，您这边需要吗？顾客:你们仓储仓储包，包括报关这些吗？客服:哦，对对，就是说仓储他是就是帮您待发货嘛，直接从咱们仓储这边发货，那这边我先帮您记录上，后续我这边直接让业务来给您那个就直接沟通就行，您目前就是咱们那个发货地的话是在那个东莞市南城街道这边吗？顾客:嗯。哦。哦。东莞。对对。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 10636, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:呃，沈万是吧，那我就要嗯。顾客:你们没有，你们有你们有空运吗？客服:可以的先生，我们这边有嗯，有那个整车快运快运服务，包括那个港澳台需求仓储，您这边需要吗？顾客:你们仓储仓储包，包括报关这些吗？客服:哦，对对，就是说仓储他是就是帮您待发货嘛，直接从咱们仓储这边发货，那这边我先帮您记录上，后续我这边直接让业务来给您那个就直接沟通就行，您目前就是咱们那个发货地的话是在那个东莞市南城街道这边吗？顾客:嗯。哦。哦。东莞。对对。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 10636, 'ans_span': []}\n",
      "空运\n",
      "{'context': '客服:到了好的那这边给您备注100公斤以内，然后您的这个送货时效有要求吗？比如说要呃除了禁地的我们要就是呃，就是正常是要正常的走件速度还是说呃，就是我们每单要啊，比如说要当日达呀，或者是说48小时必须到达这些有要求吗？送货时效。顾客:这个看客户需求。客服:行好的，那就是暂时没有特殊的这个需求勘测客户的要求了您这边有特殊的物流需求吗？比如说有的时候您需要夏天需要发视频的时候冷链服务呀，还有就是说比如寄国外港澳台这些特殊的需求，如果有的话，这边给您备注上。顾客:冷链的，有时候会可能会有吧？客服:有的好的，我这边先给您备注有的东西需要冷链，嗯，那像您有这么大的发电量呃，您这边经常合作的这个物流是哪哪个比较多呀？比如说德邦安能还是说您当地的物流多一点。顾客:嗯，我们基本上都有顺丰啊，申通啊，韵达还是啊。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 10616, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:哦，物流也用发产品。哦，那到时候让我们那我先一文件的方式给咱们简单登记，到时候，我稍后备注，让咱销售给您细讲行吗？顾客:对。对。可以。客服:嗯，行，那咱们就是说有那个国际快递吗，比如说港澳台国际之类的。顾客:国际英国的，行么，国外。客服:国外是吧，我们是有港澳台国际服务的是英国的是吧。顾客:啊。对。客服:那咱们一般公司在合作的是哪些快递呢，比如说三通一达，或者是德邦顺丰。顾客:我们我们都不都，嗯，合作顺丰比较多。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 10396, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:呃，对那个物流呢，物流有需求嘛，比如比如说需要寄到港澳台啊，或者外国哪些地方？顾客:偶尔会有。客服:嗯，好的，呃，偶尔会有是寄到国外还是就是港澳台这些地方呢？顾客:国外。客服:嗯，我我们之前就是目目前用的是哪个快递？或者是物流公司的。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['偶尔'], 'idx': 8676, 'ans_span': []}\n",
      "偶尔\n",
      "{'context': '客服:嗯，发的您是往哪里发呀？先生。顾客:美国。客服:发美国应该是可以的，我这边有一个特殊的物流需求是邮寄港澳台国际的，我给您勾选一下，然后需销售经理跟您联系的时候您可以跟他了解一下，如果要是发到美国的话是怎么收费的可以吗？我给您勾选了。顾客:好好。客服:好的那行，您还有其他需要了解的吗？先生。顾客:嗯没有了。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 8216, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:好的，那给您选这个正常的三天，您看可以吗？顾客:正常三天。客服:就是送货的时效，您这个特殊的物流需求有要求吗？比如昌储蓄球袜快运或者是整车服务这种的。顾客:嗯。有，就是比较着急的呀。客服:着急的快运明显一个，您看可以吧。顾客:嗯。客服:好的，您平时有其其他的快递吗，比如顺丰啊，或者是三通一达一类的。顾客:我基本上都是用顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['快运'], 'idx': 7266, 'ans_span': []}\n",
      "快运\n",
      "{'context': '客服:呃，走的是物流大批量那也挺多的，那我这边暂时给您勾选1到20公斤，您看可以吗？顾客:对。哦行。客服:嗯，好的，嗯，我们这边的话呢，还提供那个宽裕呀，仓储需求呀，去港澳台国际您看有这个特殊需求吗？顾客:呃仓储和什么。客服:仓储啊快运呀整车服务呀？您看有吗？这个需求。顾客:整车服务的话，我们有固定。客服:有固定的是吧，那我这边就给您勾选上，后期您要是觉得可以的话也可以跟我们这个合作，您看可以吧。顾客:嗯嗯行。客服:好的，您对我们这个送货时效要有要求吗？比如说当日达次日达三日或者无测序要求有吗？顾客:嗯，我们要不然有这种特殊要求的。我们现在用的是跨越呃跨越提示是可以选什么当日次日什么隔日陆运什么的，呃就是你能让我去选就行。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 6576, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:两公斤。哦，十公斤以下那行我给您登记1到20公斤之内，然后您这边对那个时效要求有没有？嗯要求标次日达当当日达两字哒，三日或者无要求。顾客:时效的话肯定是至少要三天以内吧，太长的话，就没有意义了。客服:哦。哦，那行那您对这物流有没有特殊需求别让仓储整车？然后这个呃冷链这块儿。呃这个。顾客:我们主要是快递，或者是物流，或者是快运。客服:哦行，那我给您备注快运快递物流这三个然后您这边过往合作的话有没有优惠？然后顺丰圆通中通这一块有没有或者京东有没有合作的？顾客:都有，我们都有合作。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['快运'], 'idx': 5286, 'ans_span': []}\n",
      "快运\n",
      "{'context': '客服:呃，资料文件文件类，那咱对这个时效的话，咱们京东时效是省内次日达全国的话三日左右，这个时效可以满足吗？顾客:可以。客服:行，您这边儿有有需要寄到国际港澳台这些特殊的需求吗？顾客:也有去寄到香港有。客服:香港有是吧，行，那我帮您登记上，您现在的话一般都是用什么快递呀。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 4886, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:呃，资料文件文件类，那咱对这个时效的话，咱们京东时效是省内次日达全国的话三日左右，这个时效可以满足吗？顾客:可以。客服:行，您这边儿有有需要寄到国际港澳台这些特殊的需求吗？顾客:也有去寄到台湾有。客服:台湾有是吧，行，那我帮您登记上，您现在的话一般都是用什么快递呀。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 4886, 'ans_span': []}\n",
      "也有\n",
      "{'context': '客服:呃，资料文件文件类，那咱对这个时效的话，咱们京东时效是省内次日达全国的话三日左右，这个时效可以满足吗？顾客:可以。客服:行，您这边儿有有需要寄到国际港澳台这些特殊的需求吗？顾客:也有去寄到澳门有。客服:澳门有是吧，行，那我帮您登记上，您现在的话一般都是用什么快递呀。顾客:顺丰。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 4886, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:啊都有优惠，嗯好的帮咱们登记一下，嗯那咱们对这个送货时效有要求吗？比如当日达次日达过三天正常时效。顾客:对。嗯，基本上三天之内吧，我都是。客服:嗯，好帮咱们记下来了，那咱们这边还有这个快运寄港澳台国际的特殊物流，咱们有这方面的需要吗？顾客:嗯有。客服:啊，是十块月，还是及港澳台国际的呀。顾客:嗯国际快递。客服:嗯，好的好的，我明白了。那我再和您核对一下，咱们的一个发货地址是在北京市通州区虞虞西一街这边吗？顾客:哦不是。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 4476, 'ans_span': []}\n",
      "港澳台国际\n",
      "{'context': '客服:噢，那我先暂时帮咱勾选在次日的，您看可以吗？顾客:嗯行。客服:好的那特殊物流有需求嘛，比如仓库快运整车服务或者说g港澳台国际这些。顾客:嗯有整车。客服:噢好的。呃，咱现在是在用哪些快递呀，或者是顺丰德邦这些吗？顾客:哎对呀。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 4296, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:哎行，好给您勾选1到20公斤啊，您对于这个送货时效有要求吗，我们有当日达次日达和一个三天的正常时效。顾客:哦，这个制药次日达就行，嗯，趁早就行。客服:嗯，次日达，诶行，好给您勾选这个次日达您对这个就特殊物流有需求吗？我们有仓储快运，包括整车还有及港澳台，您有这方面的需求吗？顾客:呃整车客服:整车是吧，好的给您勾选整车服务，然后咱们平时的话用的是顺丰比较多对吧？顾客:对嗯。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 2676, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:嗯，先生呃，好的先生就是咱个送货时效咱这边是怎么要求的？咱这边政策是两三天送到也可以当天或者第二天送的。顾客:一般都是第二天吧。客服:第二天是吧，好嘞好嘞，然后的话现在咱们咱们京东才能给您做一些增值服务先给您提供这个仓储就是全国的这个京东仓库都可以为您或者快运邮寄港澳台冷链冷链渠道这种能用的吗？顾客:仓储可以呀，仓储费用高吗？客服:仓储。呃仓储的话费用咱们挺便宜的可以让那边到时候呃这两天联系您说给您报个价你看你看下合不合适划不划算？哎好嘞好嘞，谢谢。先生就是您这边主要与顺丰快递是吗？顾客:好吧嗯。你的仓储需要我的人过去吗？不需要吧？', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 2516, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:嗯，那就给您登记三十无特殊要求正常时效可以吗？顾客:嗯，行，行行，可以。客服:嗯，您有特殊的物流需求嘛，仓储快运整车市医疗产品邮寄冷链产品港澳台国际有需求吗？顾客:运货市经常有我们经常用那个货拉拉。客服:哦就是快运吗？还是整车服务。顾客:嗯。有整车快运。客服:行，那这两个给您标记上，嗯，那您现在用的是吗？顾客:你们从。稍等别着急啊。客服:嗯好的。顾客:你们那个你们那个有有从哪儿吗？假如说我从山东我用17米长的车能否运过来嘛，应该是有吗？客服:嗯，这个可以咨询一下销售经理正常是我们这边什么车型中油的，我先把快要和整车服务给您登记的。销售经理会回答您的问题好的，您现在用的是哪一个物流或者快递呀。顾客:嗯。行行。不一定啊，你这这首先找嘛，找物流公司呢。客服:嗯，您说一下，我给您备注一下，就是都用过什么呢？顾客:就正常的物流公司当地找，你说从江苏发货，就从江苏当地找物流公司了。客服:哦，就是那种私人的是吗？顾客:发内有金属的。嗯，就建筑材料吗？客服:噢，就是私人的物流是吧，用的是就是个人的那种。顾客:对对对对。客服:哦哦好的，那就是那他给您大概多少价位啊，多少折您了解吗？顾客:多少折。客服:嗯。顾客:嗯，折那不一定谈呢？客服:哎，好的没关系的，那您有朋友需要申请这个优惠吗？可以推荐给京东一个电话吗？顾客:嗯，咱俩先谈着再说吧？行吗？客服:嗯，行好的，那已经帮您申请了，您注意接听电话就可以了，好吧。顾客:嗯，好嘞，好嘞，嗯，嗯。客服:嗯，好的，祝您生活愉快，再见。顾', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车快运'], 'idx': 2106, 'ans_span': []}\n",
      "整车快运\n",
      "{'context': '客服:嗯。顾客:你把那个玩意儿，晚上收货了，快递都是晚上收货，最迟也就是。客服:行了行。好。好京东在时效和服务商也是顶尖的，这个您放心特殊的物流有吗？比如说寄往港澳台国际的方面的冷链呢？顾客:御第三天吧。小老鼠。嗯，没有了，没有了，没有了百分之八九十。百分之八九十都是都是这样的户。客服:哦行，冷链还需要吗？顾客:嗯。客服:嗯。顾客:冷链其实不太需要。客服:因为当天就表示时效比较快的，嗯，行。顾客:金蝶放两瓶，哦，明白。客服:嗯行，另外就是再帮您核实一下，就是那个地址哈是苏州哪个区呢？顾客:振兴区。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 1106, 'ans_span': []}\n",
      "冷链\n",
      "{'context': '客服:还还是普通就可以。顾客:哦，最好快一点。客服:嗯，好的，我会帮您备注一下了。呃那咱这个特殊我特殊物流需求有吗？比如说仓储完快运呢？早上服务还有冷链产品这个记港澳台国际。顾客:嗯。呃，有冷运由冷运需求。客服:嗯，好的好的先生，那咱平时之前的话就是邮寄快递的话一般都用的什么用什么快递和物流啊。顾客:顺丰和这个和韵达。哦顺丰和圆通。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 26, 'ans_span': []}\n",
      "冷运\n",
      "{'context': '客服:好的好的，那女士想问一下，您现在都用了哪些快递呢，顺丰中通韵达之类的吗？顾客:嗯，都有云南，然后申通圆通？嗯。客服:呃，好的好的好的。他们有没有给您一个折扣价格呢？或者说是优惠券。或者是哒。好的，嗯，那您是应该没有什么特殊的物流需求吧，就例如仓储快运整车服务之类的应该都没有吧？顾客:有啊。仓储你们做仓储吗？客服:哦，我们有我们京东是有删除的，就是您把您的货放在我们那个京东的仓库里，然后有时间就是发货。这个是最快的一个。顾客:噢，那仓储呢，还是怎么计费的？客服:嗯，这个稍后我会让相关人员给您一个具体的报价我就先先先给您记录一下您这个有有有可能需要这个仓储这个仓储需求您看可以吧，好的，您对于送货的时效有什么要求吗？就例如当日达世达或者是生日无特殊需求。顾客:ok。好的好的。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 17396, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:哦，正常，那正常时效就可以是吧，嗯嗯，咱们有有没有特殊物流需求呀？比如说仓储需求啊，或者是哪方面的需求？顾客:他这个部门。嗯。有。呃我们。今年今年我们用你们这个京东的这个叫什么京东的那个叫整车那个那个业务用了，用了几次啊，用了几回。客服:噢，整车服务是吧？呃，对整车服务，嗯，嗯。嗯嗯，咱平时的话顺丰这个申通，呃那个中通都用了是吧，四通一达。顾客:嗯都。嗯，都用都用我们的文件类大多数都是顺丰顺丰，然后最近用的是京东快递，然后前一段时间用过这个京东的这个什么京东这个整车复工用了有个几十万嘛，用了十几万十几万。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 14291, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:饲料添加剂是吧，行我们京东物流的话也有这种整车再发的这个呃这种。顾客:嗯，你们你们这种发的话是直接，呃您稍等啊，我们我们是我们花的话都是直接到客户那的。客服:方式的那您这边。嗯。您您说。哦，我们也也有，如果你有这方面需求的话，到时候我们也可以安排这个师傅单独上门给您进行个详谈这个也是可以的，那您公司有没有这种仓储需求或者快运呢？或者整车这种服务的这个有吗？顾客:嗯。我们现在用的是运满满，然后整车的话一般。客服:行吗？顾客:对，运满满在，直接在app上。在他们那个app上直接找司机，然后我们那个还有几个号经常合作的物流我们那个发的价格。客服:哦这个。嗯。你们一般都是发的哪些物流公司呢？顾客:都挺急的啊。都是当地的像。不是当地的一些少物流价格便宜吗？', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 14291, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:嗯。嗯，嗯哦，这个肯定能到，嗯。顾客:嗯。几千四五天就因为怕万一路上有个什么问题，我们都会物料提前四五天五天左右邮寄吧，大概时间。客服:行，哎行，那邮寄港澳台国际这个您有要求您有要需要吗？顾客:嗯，没有，就主要是国内国内拿到会取新疆，你们的能到吗？客服:新疆也可以的，就是费用肯定是比这边高一点。顾客:哦，新疆，我们每年乌鲁木齐会邮寄一部分。客服:嗯。哦，这边也有行，那咱顺丰这边大概给您打几折呀？顾客:哦。我也搞不清楚，反正用了好久了，因为用过原来用过圆通申通什么韵达都用过后来好像用的就那几通几达的老师有的到不了完了有了几通几达会比顺丰便宜一些后来顺丰给打了两次了，就一直用他们讲了，而且也是个业务员人来上门聊的', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['没有'], 'idx': 14291, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:嗯。嗯，嗯哦，这个肯定能到，嗯。顾客:嗯。几千四五天就因为怕万一路上有个什么问题，我们都会物料提前四五天五天左右邮寄吧，大概时间。客服:行，哎行，那邮寄港澳台国际这个您有要求您有要需要吗？顾客:嗯，不用，就主要是国内国内拿到会取新疆，你们的能到吗？客服:新疆也可以的，就是费用肯定是比这边高一点。顾客:哦，新疆，我们每年乌鲁木齐会邮寄一部分。客服:嗯。哦，这边也有行，那咱顺丰这边大概给您打几折呀？顾客:哦。我也搞不清楚，反正用了好久了，因为用过原来用过圆通申通什么韵达都用过后来好像用的就那几通几达的老师有的到不了完了有了几通几达会比顺丰便宜一些后来顺丰给打了两次了，就一直用他们讲了，而且也是个业务员人来上门聊的', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不用'], 'idx': 14291, 'ans_span': []}\n",
      "不需要\n",
      "{'context': '客服:嗯。嗯，嗯哦，这个肯定能到，嗯。顾客:嗯。几千四五天就因为怕万一路上有个什么问题，我们都会物料提前四五天五天左右邮寄吧，大概时间。客服:行，哎行，那邮寄港澳台国际这个您有要求您有要需要吗？顾客:嗯，不需要，就主要是国内国内拿到会取新疆，你们的能到吗？客服:新疆也可以的，就是费用肯定是比这边高一点。顾客:哦，新疆，我们每年乌鲁木齐会邮寄一部分。客服:嗯。哦，这边也有行，那咱顺丰这边大概给您打几折呀？顾客:哦。我也搞不清楚，反正用了好久了，因为用过原来用过圆通申通什么韵达都用过后来好像用的就那几通几达的老师有的到不了完了有了几通几达会比顺丰便宜一些后来顺丰给打了两次了，就一直用他们讲了，而且也是个业务员人来上门聊的', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不需要'], 'idx': 14291, 'ans_span': []}\n",
      "不需要\n",
      "{'context': '客服:嗯。嗯，嗯哦，这个肯定能到，嗯。顾客:嗯。几千四五天就因为怕万一路上有个什么问题，我们都会物料提前四五天五天左右邮寄吧，大概时间。客服:行，哎行，那邮寄港澳台国际这个您有要求您有要需要吗？顾客:嗯，用不到，就主要是国内国内拿到会取新疆，你们的能到吗？客服:新疆也可以的，就是费用肯定是比这边高一点。顾客:哦，新疆，我们每年乌鲁木齐会邮寄一部分。客服:嗯。哦，这边也有行，那咱顺丰这边大概给您打几折呀？顾客:哦。我也搞不清楚，反正用了好久了，因为用过原来用过圆通申通什么韵达都用过后来好像用的就那几通几达的老师有的到不了完了有了几通几达会比顺丰便宜一些后来顺丰给打了两次了，就一直用他们讲了，而且也是个业务员人来上门聊的', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['用不到'], 'idx': 14291, 'ans_span': []}\n",
      "用不到\n",
      "{'context': '客服:行，那我就给您登记到三十咱的这个时效有特殊要求吗？同城当日从省次日全国三日，您看可以吗？顾客:呃这个可以。客服:噢，那需要邮寄港澳台吗，或者仓储冷链这些特殊服务。顾客:嗯，不需要我，我基本上都是山东省内。客服:嗯，明白。那您现在一般用什么快递发货呢？顾客:嗯，我们说实话，我们也有。有申通还有圆通有的时候发急件儿的话还有发京东顺丰都有这个这个呢，因为我们有几个店你肯定是经济这个店铺啊，有的有的地方有优惠啊，这个的话我们就。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不需要'], 'idx': 14291, 'ans_span': []}\n",
      "不需要\n",
      "{'context': '客服:行，那我就给您登记到三十咱的这个时效有特殊要求吗？同城当日从省次日全国三日，您看可以吗？顾客:呃这个可以。客服:噢，那需要邮寄港澳台吗，或者仓储冷链这些特殊服务。顾客:嗯，有的客人要发到台湾那边。客服:嗯，明白。那您现在一般用什么快递发货呢？顾客:嗯，我们说实话，我们也有。有申通还有圆通有的时候发急件儿的话还有发京东顺丰都有这个这个呢，因为我们有几个店你肯定是经济这个店铺啊，有的有的地方有优惠啊，这个的话我们就。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['港澳台国际'], 'idx': 14291, 'ans_span': []}\n",
      "仓储冷链\n",
      "{'context': '客服:哦。嗯，那先生你从从200到一万以上好吧。嗯，好的就说您一般一个月发多少单呢？是每天能发货嘛，比如说50到100单以内还一二百。顾客:嗯可以。不是。一两百。客服:好的好的，嗯那您平时的话这个货是呃是整车拉的吗？还是说散单的比如说快运整车发货需要吗？平时？顾客:一般都是包车的话，都是整车行吗？客服:嗯，好，呃，会为其港澳台国际呢。哦了解了解了，嗯那您这个是当地物流发货的话一般是就是用的什么价位呢，一般都是一单的话。顾客:喂。喂。但包车的话，有时候一两千。有时候大几百就那样。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 14291, 'ans_span': []}\n",
      "整车\n",
      "{'context': '客服:哦。嗯，那先生你从从200到一万以上好吧。嗯，好的就说您一般一个月发多少单呢？是每天能发货嘛，比如说50到100单以内还一二百。顾客:嗯可以。不是。一两百。客服:好的好的，嗯那您平时的话这个货是呃是整车拉的吗？还是说散单的比如说快运整车发货需要吗？平时？顾客:一般没那么多货，用不着发整车。客服:嗯，好，呃，会为其港澳台国际呢。哦了解了解了，嗯那您这个是当地物流发货的话一般是就是用的什么价位呢，一般都是一单的话。顾客:喂。喂。但包车的话，有时候一两千。有时候大几百就那样。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['用不着'], 'idx': 14291, 'ans_span': []}\n",
      "用不着\n",
      "{'context': '客服:哎，行嗯，那那您像平时需要快运和仓储这类需求吗，比如说您发货量比较大的话需要嗯京东的这些仓库呀，什么帮您存东西吗，有这方面需求吗？顾客:嗯，京东京东是不是在河南的？有个云仓啊，是不是？客服:嗯，咱们京东的话现在是全国各地方都有点儿，然后好多地方都可以提供仓储这一方面儿需求呃，如果您要有需要的话到时候，嗯，我这边帮您选择一下，到时候，让他给您报个报价，您到时候考虑考虑。顾客:哦，那那那行吧？我知道河南省漯河吧。可能有个云仓吗？客服:哦，就是您有时候也会需要这个仓库。哦哦，明白明白。嗯您之前用过其他快递没有呀，比如说呃了解过快递没有用过顺丰呀，或京东呀，用过这一类的吗？顾客:就这样，大概有很多人提起提起过这个事，对对，对。我们的产品发货，呃发财明明没有，那没有没有用过啊，我们平时用过。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 14291, 'ans_span': []}\n",
      "仓储\n",
      "{'context': '客服:哎，行嗯，那那您像平时需要快运和仓储这类需求吗，比如说您发货量比较大的话需要嗯京东的这些仓库呀，什么帮您存东西吗，有这方面需求吗？顾客:嗯，京东京东是不是在河南的？有个云仓啊，是不是？客服:嗯，咱们京东的话现在是全国各地方都有点儿，然后好多地方都可以提供仓储这一方面儿需求呃，如果您要有需要的话到时候，嗯，我这边帮您选择一下，到时候，让他给您报个报价，您到时候考虑考虑。顾客:不需要了，我就随便问问。客服:哦哦，明白明白。嗯您之前用过其他快递没有呀，比如说呃了解过快递没有用过顺丰呀，或京东呀，用过这一类的吗？顾客:就这样，大概有很多人提起提起过这个事，对对，对。我们的产品发货，呃发财明明没有，那没有没有用过啊，我们平时用过。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['不需要'], 'idx': 14291, 'ans_span': []}\n",
      "不需要\n",
      "{'context': '客服:哎三公里以内的送货时效方面有要求吗？就正常的快递速度就行，您还是说需要次日达二日达的。顾客:嗯，正常使的就行。客服:正常的公司，这边就是说目前除了就是说这个有这个云仓储的需求以外，您有这个比如说寄国外港澳台，呃需要快运服务这方面的需求，咱们有吗？顾客:现在没有。客服:没有是吗？您想您这个清洁剂药贴他发一单的货车大概在多少是100元以下吗？还是说100到200元或更贵一点的。顾客:呃，基本上都在100元以下。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 14291, 'ans_span': []}\n",
      "100元以下\n",
      "{'context': '客服:哎三公里以内的送货时效方面有要求吗？就正常的快递速度就行，您还是说需要次日达二日达的。顾客:嗯，正常使的就行。客服:正常的公司，这边就是说目您有这个比如说寄国外港澳台，呃需要快运服务这方面的需求，咱们有吗？顾客:现在没有。客服:没有是吗？您想您这个清洁剂药贴他发一单的货车大概在多少是100元以下吗？还是说100到200元或更贵一点的。顾客:呃，基本上都在100元以下。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['没有'], 'idx': 14291, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:哎三公里以内的送货时效方面有要求吗？就正常的快递速度就行，您还是说需要次日达二日达的。顾客:嗯，正常使的就行。客服:正常的公司，这边就是说目前除了就是说这个有这个仓储的需求以外，您有这个比如说寄国外港澳台，呃需要快运服务这方面的需求，咱们有吗？顾客:现在没有。客服:没有是吗？您想您这个清洁剂药贴他发一单的货车大概在多少是100元以下吗？还是说100到200元或更贵一点的。顾客:呃，基本上都在100元以下。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['仓储'], 'idx': 14291, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:哎三公里以内的送货时效方面有要求吗？就正常的快递速度就行，您还是说需要次日达二日达的。顾客:嗯，正常使的就行。客服:正常的公司，这边就是说目前除了就是说这个有这个整车的需求以外，您有这个比如说寄国外港澳台，呃需要快运服务这方面的需求，咱们有吗？顾客:现在没有。客服:没有是吗？您想您这个清洁剂药贴他发一单的货车大概在多少是100元以下吗？还是说100到200元或更贵一点的。顾客:呃，基本上都在100元以下。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['整车'], 'idx': 14291, 'ans_span': []}\n",
      "没有\n",
      "{'context': '客服:哎三公里以内的送货时效方面有要求吗？就正常的快递速度就行，您还是说需要次日达二日达的。顾客:嗯，正常使的就行。客服:正常的公司，这边就是说目前除了就是说这个有这个冷链的需求以外，您有这个比如说寄国外港澳台，呃需要快运服务这方面的需求，咱们有吗？顾客:现在没有。客服:没有是吗？您想您这个清洁剂药贴他发一单的货车大概在多少是100元以下吗？还是说100到200元或更贵一点的。顾客:呃，基本上都在100元以下。', 'question': '顾客有港澳台、国际、冷链、仓储、快运、整车需求吗，有的话是什么', 'answer': ['冷链'], 'idx': 14291, 'ans_span': []}\n",
      "没有\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    pred_ids = model.generate(input_ids=input_ids,num_beams=1,top_k=6,do_sample=True,temperature=1,num_return_sequences=1)\n",
    "    res=tokenizer.batch_decode(pred_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    res=[each.lstrip(\"<extra_id_0>\") for each in res]\n",
    "    for _,each in enumerate(zip(all[i1:i2],res)):\n",
    "        q,a=each\n",
    "        print(q)\n",
    "        # print(q['context'])\n",
    "        # print(q['answer'])\n",
    "        print(a)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['发物流这方面的需求呀', '暂时不需要', '我发的少。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7808"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "488*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "648"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('客服:嗯，现在一般就只用京东发货是吧？顾客:嗯。客服:嗯，明白。那咱这边儿这个发货地址还要从北京市昌平区这边发货是吧？顾客:对啊，昌平区回龙观。客服:回龙观镇，呃，咱是这个悦客专卖店是吧？顾客:对对对对。客服:噢，那您贵姓呀哥？顾客:石头的石。客服:啊十总行，那您的基本需求我就明白了回头我把这个就是星期一提交上去。然后最后的话销售经理给您对接这个折扣。您这边保持电话，畅通就行。顾客:行行好吧，因为你也知道我那个。因为我都指定派送员说指定哪一个。客服:嗯。顾客:一般下单都给他，呃呃，你是不是给我申请优惠卷什么呀？客服:嗯嗯。对对对这个跟那个业务员没有关系，这个销售经理会给您对接这个折扣，这边也都是总部这边统一给老客户安排，这个优惠活动。顾客:嗯，可以可以，因为我真实的运动。客服:嗯。嗯，嗯，他就是您还可以指定那个销售员给您取货呀，干什么的？顾客:对呀，我都指定随便我一个人知道吗？客服:行，那您这边保持电话畅通。顾客:因为他。我都是早点儿来完整的都无所谓，一般情况都收了，嗯。客服:行行，明白。明白。嗯。顾客:哦，哎，能不能给给个免费的那种？小不小茨河乡啊？客服:想要适合的箱子，这个我这边只能帮您登记一下，因为我们是客服部的具体的价格和这个就是箱子问题得销售经理给您对接。顾客:嗯，行行，我知道没事。没事我可以问。客服:呃，我们的权限不大就是只能负责登记一下，然后销售经理给您对接的折扣优惠啊，箱子什么的。顾客:好吧，好吧，好，好好。客服:那就先不打扰您了，祝您生活愉快。顾客:好拜拜。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('客服:两天的。哦行，那那就咱们这个时效是可以满足的，那您这边需要用到咱们京东这个仓储啊，快运呀，或者就是寄往国际港澳台这些特殊的物流需求有吗？顾客:物流。哎。嗯，这个也有，但不多像其港澳台在国国际快递不多。客服:那我就暂时先帮你登记没有这个特殊需求吧，等后期咱们有的话到时候可以单带单词再算就行啊，那您现在的一般就是发这个顺丰的话，呃都是发一些急件儿小件儿，那些那物流的话，一般走什么物流啊。顾客:嗯嗯。物流就好，比说嗯，都是专线好，比说。嗯，从浙江发往安徽安徽专线发往山东有山东专线。客服:呃，呃，就是专门儿这个地区这个专线是吧？噢噢。顾客:对他都是专线，所以说专业嘛也有，哎说白了他也他也要中转的。客服:这样子是吧，行嗯，那您现在跟顺丰这边您是有合作嘛，就是也是是有这个月结呃账户吗？还是平时发一份呀。顾客:嗯，我们跟顺丰寄寄快递，这些是是月结账户。单位记账呢。客服:哦。行，咱们因为京东的话我们现在也是对外开放也是想要寻求一些就是能够合作的月结这个呃客户嘛，然后如果咱们京东也合适的话，您这边呃。好可以可以和京东这边就是天月结行是吧？顾客:嗯，也可以的这个看嘛，因为我看现在很多这个物流快递这一块京东的还蛮普遍的，媳妇的京东快递也挺多的。客服:也可以是吧。对对对。啊是的是的，嗯行，那您像一般像这个呃顺丰呀，那就咱们发这个小件货的话，咱一般能够接受的物流费用的话大概要多少钱呢？顾客:他这不等你要我们寄了寄这个就是说一个一个文件啊，不超重一个一个小文件一两公斤一公斤样子凳子一共金都没有零点几公斤，一个小一个，顺丰快递盒。嗯寄往安徽。单量是我们寄顺丰的是12块5毛钱。客服:12块5毛钱。顾客:我也不知道，你我不知道你们嗯，你们京东大概要寄到安徽合肥大概多少钱，就浙江安吉。客服:哦行，行那我可以让我们这边给您发个报价咱们看一下啊，那您这边如果跟物流公司合作的话咱比较就是看中哪方面的这个呃就是服务呀，比如说价格呀服务呀时效呀，您比较注重哪方面呀。顾客:一般都是像物流这一块我们也没有也没有说嗯，特大要求因为大家都知道的嘛，走物流都是人家有大车走也不可能就单单位你这一点货可以单独跑一趟，对不对？我们你都了解了好比说我们正常发货，嗯今天发。客服:嗯嗯。顾客:发过去的话我们就打个电话比方说我们货都把打个电话你们来了，你们来给我拉走。嗯，那时候也都2到三两到三天能到达目的地就可以了。客服:哦哦就是。顾客:没有别的要求。客服:行行，就基本上的话，咱应该还是比较尊重这个服务呀，时效方面的是吧？顾客:失效满说实话对一定要要也也在与观念几乎我们都是为了就是说嗯，不让或损坏呀或怎么样？因为我们那个布料都是双层包装应该一般也都不会就怕陵水陵与下雨天的了吧？最怕的下雨天。客服:哦。嗯，主要就是在物流服务方面吧。顾客:对，对，对其他的没什么要求？客服:哦行。行行咱发货，嗯。顾客:价格吗，几乎价格几乎现在都透明了你报告了也没用呢？大家一一比方说你报比我高那别人家的比你便宜物流啊，就是服务态度都差不多，那我还不如原来的快递对不对这个东西价格几乎都不透明化的？现在都差不多。客服:对对。嗯，是是咱发货是在这个安吉县这边儿是吗？顾客:嗯。不等我想好，比说我人在按几那个货发货有可能在嘉兴有可能在桐乡也有可能在上海不也不等的，因为我们是做贸易的。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
