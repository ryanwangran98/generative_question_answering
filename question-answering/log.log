[2023-07-12 20:02:32,599] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-07-12 20:03:00,490] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-07-12 20:03:00,498] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-07-12 20:03:00,519] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-07-12 20:03:00,528] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
07/12/2023 20:03:14 - INFO - __main__ - Distributed environment: DEEPSPEED
Num processes: 4
Process index: 3
Local process index: 3
Device: cuda:3

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

07/12/2023 20:03:14 - INFO - __main__ - Distributed environment: DEEPSPEED
Num processes: 4
Process index: 1
Local process index: 1
Device: cuda:1

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

07/12/2023 20:03:14 - INFO - __main__ - Distributed environment: DEEPSPEED
Num processes: 4
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

07/12/2023 20:03:14 - INFO - __main__ - Distributed environment: DEEPSPEED
Num processes: 4
Process index: 2
Local process index: 2
Device: cuda:2

Mixed precision type: bf16
ds_config: {'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'gradient_accumulation_steps': 1, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu'}, 'offload_param': {'device': 'cpu'}, 'stage3_gather_16bit_weights_on_model_save': False}, 'steps_per_print': inf, 'bf16': {'enabled': True}, 'fp16': {'enabled': False}}

  0%|          | 0/2 [00:00<?, ?it/s]07/12/2023 20:04:40 - WARNING - datasets.builder - Found cached dataset json (/media/cfs/wangran108/question-answering/huggingface/datasets/json/default-53829690f5b5842e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 108.62it/s]
100%|██████████| 2/2 [00:00<00:00, 128.94it/s]
07/12/2023 20:04:40 - INFO - __main__ - load dataset done.........
Offline mode: forcing local_files_only=True
loading configuration file /home/wangran108/code/model_file/randengt5/config.json
Model config MT5Config {
  "_name_or_path": "/home/wangran108/code/model_file/randengt5",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 32601
}

Offline mode: forcing local_files_only=True
Offline mode: forcing local_files_only=True
loading file spiece.model
loading file tokenizer.json
loading file added_tokens.json
loading file special_tokens_map.json
loading file tokenizer_config.json
  0%|          | 0/2 [00:00<?, ?it/s]  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 210.14it/s]
100%|██████████| 2/2 [00:00<00:00, 198.35it/s]
Adding [CTSTART] to the vocabulary
Adding [CTEND] to the vocabulary
Adding [SEP] to the vocabulary
Adding [KNSTART] to the vocabulary
Adding [KNEND] to the vocabulary
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/12/2023 20:04:40 - INFO - __main__ - load model begin.........
Offline mode: forcing local_files_only=True
loading configuration file /home/wangran108/code/model_file/randengt5/config.json
Model config MT5Config {
  "_name_or_path": "/home/wangran108/code/model_file/randengt5",
  "architectures": [
    "MT5ForConditionalGeneration"
  ],
  "d_ff": 2816,
  "d_kv": 64,
  "d_model": 1024,
  "decoder_start_token_id": 0,
  "dense_act_fn": "gelu_new",
  "dropout_rate": 0.1,
  "eos_token_id": 1,
  "feed_forward_proj": "gated-gelu",
  "initializer_factor": 1.0,
  "is_encoder_decoder": true,
  "is_gated_act": true,
  "layer_norm_epsilon": 1e-06,
  "model_type": "mt5",
  "num_decoder_layers": 24,
  "num_heads": 16,
  "num_layers": 24,
  "output_past": true,
  "pad_token_id": 0,
  "relative_attention_max_distance": 128,
  "relative_attention_num_buckets": 32,
  "tie_word_embeddings": false,
  "tokenizer_class": "T5Tokenizer",
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 32601
}

/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Offline mode: forcing local_files_only=True
loading weights file /home/wangran108/code/model_file/randengt5/pytorch_model.bin
Generate config GenerationConfig {
  "_from_model_config": true,
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

All model checkpoint weights were used when initializing MT5ForConditionalGeneration.

All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at /home/wangran108/code/model_file/randengt5.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.
Generation config file not found, using a generation config created from the model config.
07/12/2023 20:04:55 - INFO - __main__ - load model done.........
07/12/2023 20:04:55 - INFO - __main__ - False
07/12/2023 20:04:55 - INFO - __main__ - start preprocess_function .........
07/12/2023 20:04:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /media/cfs/wangran108/question-answering/huggingface/datasets/json/default-53829690f5b5842e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-297b14208f39badb_*_of_00012.arrow
07/12/2023 20:04:56 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /media/cfs/wangran108/question-answering/huggingface/datasets/json/default-53829690f5b5842e/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-84baec972d30c62e_*_of_00012.arrow
07/12/2023 20:04:56 - INFO - __main__ - Sample 5425 of the training set: {'input_ids': [259, 12877, 267, 259, 25601, 938, 3948, 15295, 261, 938, 3948, 9615, 259, 7153, 267, 259, 8338, 267, 11087, 261, 2907, 5695, 2184, 6209, 408, 1316, 6023, 19167, 13317, 3989, 286, 12738, 1773, 3989, 11144, 1129, 2298, 542, 1129, 2830, 542, 1129, 921, 542, 7487, 766, 3377, 10607, 3989, 297, 2638, 4971, 11087, 261, 2907, 19167, 1708, 27541, 13993, 1708, 5772, 5214, 5898, 261, 12738, 1773, 3989, 810, 4093, 20509, 261, 27214, 13993, 20509, 261, 3917, 1053, 3217, 850, 20509, 261, 4481, 13317, 10607, 3989, 286, 25601, 267, 26896, 261, 2907, 5245, 3377, 10607, 3989, 1835, 286, 3377, 297, 8338, 267, 3377, 6865, 27538, 11378, 794, 2172, 5098, 408, 3617, 15295, 1835, 286, 12738, 1773, 5783, 5601, 2300, 8475, 921, 1565, 709, 3111, 20509, 261, 27538, 2172, 9483, 2375, 286, 25601, 267, 5783, 5601, 6110, 297, 8338, 267, 5783, 5601, 8318, 1216, 2824, 19167, 794, 8385, 1835, 286, 25601, 267, 26896, 297, 8338, 267, 794, 6865, 766, 22941, 5898, 286, 11087, 261, 2907, 1182, 1708, 4450, 11378, 5183, 19167, 3617, 22474, 19682, 17397, 9614, 6306, 20509, 261, 5688, 2907, 4450, 5214, 27538, 11378, 24317, 6439, 7296, 1216, 9771, 1097, 2023, 935, 261, 19167, 1708, 4450, 5214, 27538, 3617, 1837, 6209, 408, 15556, 11378, 5048, 617, 3398, 2462, 1279, 1835, 286, 1475, 1318, 297, 2515, 17971, 11087, 261, 2907, 19167, 3617, 4643, 1344, 13267, 696, 358, 2326, 10404, 7047, 1129, 5688, 1835, 286, 25601, 267, 2515, 17971, 297, 26896, 261, 22340, 25911, 13267, 3321, 297, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [259, 5783, 5601, 1]}.
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
07/12/2023 20:04:56 - INFO - accelerate.accelerator - Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (4).
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
/media/cfs/wangran108/.pylib/lib/python3.8/site-packages/accelerate/accelerator.py:498: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.
  warnings.warn(
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /media/cfs/wangran108/question-answering/torch_extensions/py38_cu117 as PyTorch extensions root...
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /media/cfs/wangran108/question-answering/torch_extensions/py38_cu117 as PyTorch extensions root...
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /media/cfs/wangran108/question-answering/torch_extensions/py38_cu117 as PyTorch extensions root...
Installed CUDA version 11.1 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /media/cfs/wangran108/question-answering/torch_extensions/py38_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /media/cfs/wangran108/question-answering/torch_extensions/py38_cu117/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 5.38818883895874 seconds
Loading extension module cpu_adam...Loading extension module cpu_adam...

Time to load cpu_adam op: 5.4672017097473145 seconds
Time to load cpu_adam op: 5.467052936553955 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 5.482303619384766 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2023-07-12 20:05:19,858] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2023-07-12 20:05:19,859] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-12 20:05:19,859] [INFO] [comm.py:594:init_distributed] cdb=None
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2023-07-12 20:05:19,884] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2023-07-12 20:05:19,885] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-12 20:05:19,885] [INFO] [comm.py:594:init_distributed] cdb=None
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2023-07-12 20:05:20,009] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2023-07-12 20:05:20,009] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-12 20:05:20,009] [INFO] [comm.py:594:init_distributed] cdb=None
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2023-07-12 20:05:20,296] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.9.5, git-hash=unknown, git-branch=unknown
[2023-07-12 20:05:20,297] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-07-12 20:05:20,297] [INFO] [comm.py:594:init_distributed] cdb=None
07/12/2023 20:05:21 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 2
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 0
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 1
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:2 to store for rank: 3
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
07/12/2023 20:05:22 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 4 nodes.
[2023-07-12 20:05:22,724] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-07-12 20:05:22,731] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-07-12 20:05:22,731] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-07-12 20:05:22,806] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2023-07-12 20:05:22,806] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2023-07-12 20:05:22,807] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-07-12 20:05:22,807] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-07-12 20:05:22,807] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-07-12 20:05:22,807] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: True
[2023-07-12 20:05:22,807] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 2 partition count [4, 4] and sizes[(195998208, False), (31488, False)] 
Rank: 3 partition count [4, 4] and sizes[(195998208, False), (31488, False)] 
Rank: 0 partition count [4, 4] and sizes[(195998208, False), (31488, False)] 
Rank: 1 partition count [4, 4] and sizes[(195998208, False), (31488, False)] 
[2023-07-12 20:05:28,721] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-07-12 20:05:28,755] [INFO] [utils.py:786:see_memory_usage] MA 1.52 GB         Max_MA 1.52 GB         CA 1.53 GB         Max_CA 2 GB 
[2023-07-12 20:05:28,756] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 82.11 GB, percent = 32.7%
[2023-07-12 20:05:32,448] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-07-12 20:05:32,449] [INFO] [utils.py:786:see_memory_usage] MA 1.52 GB         Max_MA 1.52 GB         CA 1.53 GB         Max_CA 2 GB 
[2023-07-12 20:05:32,450] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 90.38 GB, percent = 35.9%
[2023-07-12 20:05:32,450] [INFO] [stage_1_and_2.py:488:__init__] optimizer state initialized
[2023-07-12 20:05:32,692] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-07-12 20:05:32,693] [INFO] [utils.py:786:see_memory_usage] MA 1.52 GB         Max_MA 1.52 GB         CA 1.53 GB         Max_CA 2 GB 
[2023-07-12 20:05:32,694] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 90.39 GB, percent = 35.9%
[2023-07-12 20:05:32,707] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2023-07-12 20:05:32,708] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-07-12 20:05:32,708] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-07-12 20:05:32,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2023-07-12 20:05:32,713] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-07-12 20:05:32,713] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-07-12 20:05:32,713] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-07-12 20:05:32,713] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   amp_params ................... False
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f40236b8160>
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-07-12 20:05:32,714] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   dump_state ................... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-07-12 20:05:32,715] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-07-12 20:05:32,716] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   pld_params ................... False
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   train_batch_size ............. 16
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  4
[2023-07-12 20:05:32,717] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   world_size ................... 4
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-07-12 20:05:32,718] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-07-12 20:05:32,718] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu"
        }, 
        "offload_param": {
            "device": "cpu"
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
07/12/2023 20:05:33 - INFO - __main__ - get metric done.........
07/12/2023 20:05:33 - INFO - __main__ - ***** Running training *****
07/12/2023 20:05:33 - INFO - __main__ -   Num examples = 7268
07/12/2023 20:05:33 - INFO - __main__ -   Num Epochs = 1
07/12/2023 20:05:33 - INFO - __main__ -   Instantaneous batch size per device = 4
07/12/2023 20:05:33 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
07/12/2023 20:05:33 - INFO - __main__ -   Gradient Accumulation steps = 1
07/12/2023 20:05:33 - INFO - __main__ -   Total optimization steps = 455
  0%|          | 0/455 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/455 [00:03<23:25,  3.10s/it]07/12/2023 20:05:36 - INFO - __main__ - {'train_loss': 16.75, 'epoch': 0, 'step': 1}
  0%|          | 2/455 [00:06<23:55,  3.17s/it]07/12/2023 20:05:39 - INFO - __main__ - {'train_loss': 18.75, 'epoch': 0, 'step': 2}
  1%|          | 3/455 [00:09<24:38,  3.27s/it]07/12/2023 20:05:42 - INFO - __main__ - {'train_loss': 16.229166666666668, 'epoch': 0, 'step': 3}
  1%|          | 4/455 [00:12<24:26,  3.25s/it]07/12/2023 20:05:46 - INFO - __main__ - {'train_loss': 18.609375, 'epoch': 0, 'step': 4}
  1%|          | 5/455 [00:16<24:16,  3.24s/it]07/12/2023 20:05:49 - INFO - __main__ - {'train_loss': 20.0625, 'epoch': 0, 'step': 5}
  1%|▏         | 6/455 [00:19<23:36,  3.15s/it]07/12/2023 20:05:52 - INFO - __main__ - {'train_loss': 19.96875, 'epoch': 0, 'step': 6}
  2%|▏         | 7/455 [00:22<23:46,  3.18s/it]07/12/2023 20:05:55 - INFO - __main__ - {'train_loss': 19.0, 'epoch': 0, 'step': 7}
  2%|▏         | 8/455 [00:25<23:49,  3.20s/it]07/12/2023 20:05:58 - INFO - __main__ - {'train_loss': 18.3984375, 'epoch': 0, 'step': 8}
  2%|▏         | 9/455 [00:28<24:04,  3.24s/it]07/12/2023 20:06:02 - INFO - __main__ - {'train_loss': 18.20138888888889, 'epoch': 0, 'step': 9}
  2%|▏         | 10/455 [00:32<23:54,  3.22s/it]07/12/2023 20:06:05 - INFO - __main__ - {'train_loss': 18.18125, 'epoch': 0, 'step': 10}
  2%|▏         | 11/455 [00:35<23:41,  3.20s/it]07/12/2023 20:06:08 - INFO - __main__ - {'train_loss': 17.886363636363637, 'epoch': 0, 'step': 11}
  3%|▎         | 12/455 [00:38<23:03,  3.12s/it]07/12/2023 20:06:11 - INFO - __main__ - {'train_loss': 17.044270833333332, 'epoch': 0, 'step': 12}
  3%|▎         | 13/455 [00:41<23:09,  3.14s/it]07/12/2023 20:06:14 - INFO - __main__ - {'train_loss': 16.603365384615383, 'epoch': 0, 'step': 13}
  3%|▎         | 14/455 [00:44<23:30,  3.20s/it]07/12/2023 20:06:17 - INFO - __main__ - {'train_loss': 16.363839285714285, 'epoch': 0, 'step': 14}
  3%|▎         | 15/455 [00:47<22:48,  3.11s/it]07/12/2023 20:06:20 - INFO - __main__ - {'train_loss': 16.339583333333334, 'epoch': 0, 'step': 15}
  4%|▎         | 16/455 [00:50<22:30,  3.08s/it]07/12/2023 20:06:23 - INFO - __main__ - {'train_loss': 16.193359375, 'epoch': 0, 'step': 16}
  4%|▎         | 17/455 [00:53<22:57,  3.14s/it]07/12/2023 20:06:27 - INFO - __main__ - {'train_loss': 16.159926470588236, 'epoch': 0, 'step': 17}
  4%|▍         | 18/455 [00:56<22:22,  3.07s/it]07/12/2023 20:06:30 - INFO - __main__ - {'train_loss': 15.928819444444445, 'epoch': 0, 'step': 18}
  4%|▍         | 19/455 [00:59<22:19,  3.07s/it]07/12/2023 20:06:33 - INFO - __main__ - {'train_loss': 16.21546052631579, 'epoch': 0, 'step': 19}
  4%|▍         | 20/455 [01:03<22:24,  3.09s/it]07/12/2023 20:06:36 - INFO - __main__ - {'train_loss': 15.9703125, 'epoch': 0, 'step': 20}
  5%|▍         | 21/455 [01:06<22:23,  3.10s/it]07/12/2023 20:06:39 - INFO - __main__ - {'train_loss': 15.712797619047619, 'epoch': 0, 'step': 21}
  5%|▍         | 22/455 [01:09<22:50,  3.17s/it]07/12/2023 20:06:42 - INFO - __main__ - {'train_loss': 15.651988636363637, 'epoch': 0, 'step': 22}
  5%|▌         | 23/455 [01:12<22:37,  3.14s/it]07/12/2023 20:06:45 - INFO - __main__ - {'train_loss': 15.612771739130435, 'epoch': 0, 'step': 23}
  5%|▌         | 24/455 [01:15<22:38,  3.15s/it]07/12/2023 20:06:48 - INFO - __main__ - {'train_loss': 15.506510416666666, 'epoch': 0, 'step': 24}
  5%|▌         | 25/455 [01:19<22:53,  3.19s/it]07/12/2023 20:06:52 - INFO - __main__ - {'train_loss': 15.40375, 'epoch': 0, 'step': 25}
  6%|▌         | 26/455 [01:22<22:37,  3.16s/it]07/12/2023 20:06:55 - INFO - __main__ - {'train_loss': 15.424278846153847, 'epoch': 0, 'step': 26}
  6%|▌         | 27/455 [01:25<22:25,  3.14s/it]07/12/2023 20:06:58 - INFO - __main__ - {'train_loss': 15.269675925925926, 'epoch': 0, 'step': 27}
  6%|▌         | 28/455 [01:28<22:56,  3.22s/it]07/12/2023 20:07:01 - INFO - __main__ - {'train_loss': 15.052455357142858, 'epoch': 0, 'step': 28}
  6%|▋         | 29/455 [01:31<22:36,  3.18s/it]07/12/2023 20:07:04 - INFO - __main__ - {'train_loss': 14.917025862068966, 'epoch': 0, 'step': 29}
  7%|▋         | 30/455 [01:35<22:44,  3.21s/it]07/12/2023 20:07:08 - INFO - __main__ - {'train_loss': 14.803125, 'epoch': 0, 'step': 30}
  7%|▋         | 31/455 [01:38<22:44,  3.22s/it]07/12/2023 20:07:11 - INFO - __main__ - {'train_loss': 14.710685483870968, 'epoch': 0, 'step': 31}
  7%|▋         | 32/455 [01:41<22:58,  3.26s/it]07/12/2023 20:07:14 - INFO - __main__ - {'train_loss': 14.7666015625, 'epoch': 0, 'step': 32}
  7%|▋         | 33/455 [01:44<22:56,  3.26s/it]07/12/2023 20:07:18 - INFO - __main__ - {'train_loss': 14.622159090909092, 'epoch': 0, 'step': 33}
  7%|▋         | 34/455 [01:48<22:56,  3.27s/it]07/12/2023 20:07:21 - INFO - __main__ - {'train_loss': 14.561580882352942, 'epoch': 0, 'step': 34}
  8%|▊         | 35/455 [01:51<22:44,  3.25s/it]07/12/2023 20:07:24 - INFO - __main__ - {'train_loss': 14.433035714285714, 'epoch': 0, 'step': 35}
  8%|▊         | 36/455 [01:54<22:39,  3.25s/it]07/12/2023 20:07:27 - INFO - __main__ - {'train_loss': 14.405381944444445, 'epoch': 0, 'step': 36}
  8%|▊         | 37/455 [01:57<22:53,  3.28s/it]07/12/2023 20:07:31 - INFO - __main__ - {'train_loss': 14.38429054054054, 'epoch': 0, 'step': 37}
  8%|▊         | 38/455 [02:01<22:40,  3.26s/it]07/12/2023 20:07:34 - INFO - __main__ - {'train_loss': 14.354440789473685, 'epoch': 0, 'step': 38}
  9%|▊         | 39/455 [02:04<22:52,  3.30s/it]07/12/2023 20:07:37 - INFO - __main__ - {'train_loss': 14.338942307692308, 'epoch': 0, 'step': 39}
  9%|▉         | 40/455 [02:07<23:03,  3.33s/it]07/12/2023 20:07:41 - INFO - __main__ - {'train_loss': 14.35390625, 'epoch': 0, 'step': 40}
  9%|▉         | 41/455 [02:11<22:49,  3.31s/it]07/12/2023 20:07:44 - INFO - __main__ - {'train_loss': 14.330030487804878, 'epoch': 0, 'step': 41}
  9%|▉         | 42/455 [02:14<22:08,  3.22s/it]07/12/2023 20:07:47 - INFO - __main__ - {'train_loss': 14.213541666666666, 'epoch': 0, 'step': 42}
  9%|▉         | 43/455 [02:17<22:17,  3.25s/it]07/12/2023 20:07:50 - INFO - __main__ - {'train_loss': 14.073401162790697, 'epoch': 0, 'step': 43}
 10%|▉         | 44/455 [02:20<22:15,  3.25s/it]07/12/2023 20:07:54 - INFO - __main__ - {'train_loss': 14.068892045454545, 'epoch': 0, 'step': 44}
 10%|▉         | 45/455 [02:23<21:52,  3.20s/it]07/12/2023 20:07:57 - INFO - __main__ - {'train_loss': 13.972916666666666, 'epoch': 0, 'step': 45}
 10%|█         | 46/455 [02:27<21:43,  3.19s/it]07/12/2023 20:08:00 - INFO - __main__ - {'train_loss': 13.890625, 'epoch': 0, 'step': 46}
 10%|█         | 47/455 [02:30<21:39,  3.19s/it]07/12/2023 20:08:03 - INFO - __main__ - {'train_loss': 13.744015957446809, 'epoch': 0, 'step': 47}
 11%|█         | 48/455 [02:33<21:22,  3.15s/it]07/12/2023 20:08:06 - INFO - __main__ - {'train_loss': 13.742838541666666, 'epoch': 0, 'step': 48}
 11%|█         | 49/455 [02:36<21:28,  3.17s/it]07/12/2023 20:08:09 - INFO - __main__ - {'train_loss': 13.619260204081632, 'epoch': 0, 'step': 49}
 11%|█         | 50/455 [02:39<21:43,  3.22s/it]07/12/2023 20:08:13 - INFO - __main__ - {'train_loss': 13.591875, 'epoch': 0, 'step': 50}
 11%|█         | 51/455 [02:42<21:24,  3.18s/it]07/12/2023 20:08:16 - INFO - __main__ - {'train_loss': 13.490808823529411, 'epoch': 0, 'step': 51}
 11%|█▏        | 52/455 [02:45<21:00,  3.13s/it]07/12/2023 20:08:19 - INFO - __main__ - {'train_loss': 13.438100961538462, 'epoch': 0, 'step': 52}
 12%|█▏        | 53/455 [02:49<21:03,  3.14s/it]07/12/2023 20:08:22 - INFO - __main__ - {'train_loss': 13.340212264150944, 'epoch': 0, 'step': 53}
 12%|█▏        | 54/455 [02:52<20:41,  3.10s/it]07/12/2023 20:08:25 - INFO - __main__ - {'train_loss': 13.35474537037037, 'epoch': 0, 'step': 54}
 12%|█▏        | 55/455 [02:55<20:43,  3.11s/it]07/12/2023 20:08:28 - INFO - __main__ - {'train_loss': 13.261931818181818, 'epoch': 0, 'step': 55}
 12%|█▏        | 56/455 [02:58<20:36,  3.10s/it]07/12/2023 20:08:31 - INFO - __main__ - {'train_loss': 13.159598214285714, 'epoch': 0, 'step': 56}
 13%|█▎        | 57/455 [03:01<20:41,  3.12s/it]07/12/2023 20:08:34 - INFO - __main__ - {'train_loss': 13.007675438596491, 'epoch': 0, 'step': 57}
 13%|█▎        | 58/455 [03:04<20:25,  3.09s/it]07/12/2023 20:08:37 - INFO - __main__ - {'train_loss': 12.872844827586206, 'epoch': 0, 'step': 58}
 13%|█▎        | 59/455 [03:07<20:09,  3.06s/it]07/12/2023 20:08:40 - INFO - __main__ - {'train_loss': 12.760593220338983, 'epoch': 0, 'step': 59}
 13%|█▎        | 60/455 [03:10<20:38,  3.14s/it]07/12/2023 20:08:44 - INFO - __main__ - {'train_loss': 12.646875, 'epoch': 0, 'step': 60}
 13%|█▎        | 61/455 [03:13<20:33,  3.13s/it]07/12/2023 20:08:47 - INFO - __main__ - {'train_loss': 12.515368852459016, 'epoch': 0, 'step': 61}
 14%|█▎        | 62/455 [03:17<20:50,  3.18s/it]07/12/2023 20:08:50 - INFO - __main__ - {'train_loss': 12.392137096774194, 'epoch': 0, 'step': 62}
 14%|█▍        | 63/455 [03:20<21:22,  3.27s/it]07/12/2023 20:08:53 - INFO - __main__ - {'train_loss': 12.26388888888889, 'epoch': 0, 'step': 63}
 14%|█▍        | 64/455 [03:24<22:04,  3.39s/it]07/12/2023 20:08:57 - INFO - __main__ - {'train_loss': 12.14453125, 'epoch': 0, 'step': 64}
 14%|█▍        | 65/455 [03:27<21:44,  3.34s/it]07/12/2023 20:09:00 - INFO - __main__ - {'train_loss': 12.034134615384616, 'epoch': 0, 'step': 65}
 15%|█▍        | 66/455 [03:30<21:27,  3.31s/it]07/12/2023 20:09:04 - INFO - __main__ - {'train_loss': 11.911931818181818, 'epoch': 0, 'step': 66}
 15%|█▍        | 67/455 [03:34<21:37,  3.34s/it]07/12/2023 20:09:07 - INFO - __main__ - {'train_loss': 11.76445895522388, 'epoch': 0, 'step': 67}
 15%|█▍        | 68/455 [03:37<21:38,  3.36s/it]07/12/2023 20:09:10 - INFO - __main__ - {'train_loss': 11.646829044117647, 'epoch': 0, 'step': 68}
 15%|█▌        | 69/455 [03:40<21:27,  3.34s/it]07/12/2023 20:09:14 - INFO - __main__ - {'train_loss': 11.521286231884059, 'epoch': 0, 'step': 69}
 15%|█▌        | 70/455 [03:44<21:18,  3.32s/it]07/12/2023 20:09:17 - INFO - __main__ - {'train_loss': 11.400446428571428, 'epoch': 0, 'step': 70}
 16%|█▌        | 71/455 [03:47<21:00,  3.28s/it]07/12/2023 20:09:20 - INFO - __main__ - {'train_loss': 11.287632042253522, 'epoch': 0, 'step': 71}
 16%|█▌        | 72/455 [03:50<21:06,  3.31s/it]07/12/2023 20:09:23 - INFO - __main__ - {'train_loss': 11.176215277777779, 'epoch': 0, 'step': 72}
 16%|█▌        | 73/455 [03:54<20:58,  3.30s/it]07/12/2023 20:09:27 - INFO - __main__ - {'train_loss': 11.046446917808218, 'epoch': 0, 'step': 73}
 16%|█▋        | 74/455 [03:56<20:09,  3.17s/it]07/12/2023 20:09:30 - INFO - __main__ - {'train_loss': 10.935388513513514, 'epoch': 0, 'step': 74}
 16%|█▋        | 75/455 [03:59<19:39,  3.10s/it]07/12/2023 20:09:33 - INFO - __main__ - {'train_loss': 10.82125, 'epoch': 0, 'step': 75}
 17%|█▋        | 76/455 [04:02<19:25,  3.08s/it]07/12/2023 20:09:36 - INFO - __main__ - {'train_loss': 10.706003289473685, 'epoch': 0, 'step': 76}
 17%|█▋        | 77/455 [04:06<19:38,  3.12s/it]07/12/2023 20:09:39 - INFO - __main__ - {'train_loss': 10.590401785714286, 'epoch': 0, 'step': 77}
 17%|█▋        | 78/455 [04:09<19:56,  3.17s/it]07/12/2023 20:09:42 - INFO - __main__ - {'train_loss': 10.475761217948717, 'epoch': 0, 'step': 78}
 17%|█▋        | 79/455 [04:12<20:13,  3.23s/it]07/12/2023 20:09:45 - INFO - __main__ - {'train_loss': 10.35067246835443, 'epoch': 0, 'step': 79}
 18%|█▊        | 80/455 [04:15<19:18,  3.09s/it]07/12/2023 20:09:48 - INFO - __main__ - {'train_loss': 10.2396484375, 'epoch': 0, 'step': 80}
 18%|█▊        | 81/455 [04:18<18:37,  2.99s/it]07/12/2023 20:09:51 - INFO - __main__ - {'train_loss': 10.128761574074074, 'epoch': 0, 'step': 81}
 18%|█▊        | 82/455 [04:21<18:08,  2.92s/it]07/12/2023 20:09:54 - INFO - __main__ - {'train_loss': 10.022865853658537, 'epoch': 0, 'step': 82}
 18%|█▊        | 83/455 [04:23<17:52,  2.88s/it]07/12/2023 20:09:57 - INFO - __main__ - {'train_loss': 9.91105045180723, 'epoch': 0, 'step': 83}
 18%|█▊        | 84/455 [04:26<18:06,  2.93s/it]07/12/2023 20:10:00 - INFO - __main__ - {'train_loss': 9.813244047619047, 'epoch': 0, 'step': 84}
 19%|█▊        | 85/455 [04:29<18:16,  2.96s/it]07/12/2023 20:10:03 - INFO - __main__ - {'train_loss': 9.71452205882353, 'epoch': 0, 'step': 85}
 19%|█▉        | 86/455 [04:32<17:29,  2.84s/it]07/12/2023 20:10:05 - INFO - __main__ - {'train_loss': 9.609465843023257, 'epoch': 0, 'step': 86}
 19%|█▉        | 87/455 [04:35<17:21,  2.83s/it]07/12/2023 20:10:08 - INFO - __main__ - {'train_loss': 9.507767600574713, 'epoch': 0, 'step': 87}
 19%|█▉        | 88/455 [04:38<17:43,  2.90s/it]07/12/2023 20:10:11 - INFO - __main__ - {'train_loss': 9.416148792613637, 'epoch': 0, 'step': 88}
 20%|█▉        | 89/455 [04:41<17:23,  2.85s/it]07/12/2023 20:10:14 - INFO - __main__ - {'train_loss': 9.322199789325843, 'epoch': 0, 'step': 89}
 20%|█▉        | 90/455 [04:43<16:51,  2.77s/it]07/12/2023 20:10:16 - INFO - __main__ - {'train_loss': 9.22834201388889, 'epoch': 0, 'step': 90}
 20%|██        | 91/455 [04:46<17:11,  2.83s/it]07/12/2023 20:10:19 - INFO - __main__ - {'train_loss': 9.145647321428571, 'epoch': 0, 'step': 91}
 20%|██        | 92/455 [04:49<17:32,  2.90s/it]07/12/2023 20:10:22 - INFO - __main__ - {'train_loss': 9.057362432065217, 'epoch': 0, 'step': 92}
 20%|██        | 93/455 [04:53<18:32,  3.07s/it]07/12/2023 20:10:26 - INFO - __main__ - {'train_loss': 8.964570732526882, 'epoch': 0, 'step': 93}
 21%|██        | 94/455 [04:56<19:02,  3.16s/it]07/12/2023 20:10:29 - INFO - __main__ - {'train_loss': 8.875394780585106, 'epoch': 0, 'step': 94}
 21%|██        | 95/455 [05:00<19:39,  3.28s/it]07/12/2023 20:10:33 - INFO - __main__ - {'train_loss': 8.80647615131579, 'epoch': 0, 'step': 95}
 21%|██        | 96/455 [05:03<19:44,  3.30s/it]07/12/2023 20:10:36 - INFO - __main__ - {'train_loss': 8.722798665364584, 'epoch': 0, 'step': 96}
 21%|██▏       | 97/455 [05:06<20:05,  3.37s/it]07/12/2023 20:10:40 - INFO - __main__ - {'train_loss': 8.640161887886597, 'epoch': 0, 'step': 97}
 22%|██▏       | 98/455 [05:10<19:40,  3.31s/it]07/12/2023 20:10:43 - INFO - __main__ - {'train_loss': 8.568977200255102, 'epoch': 0, 'step': 98}
 22%|██▏       | 99/455 [05:13<19:53,  3.35s/it]07/12/2023 20:10:46 - INFO - __main__ - {'train_loss': 8.485144412878787, 'epoch': 0, 'step': 99}
 22%|██▏       | 100/455 [05:16<19:50,  3.35s/it]07/12/2023 20:10:50 - INFO - __main__ - {'train_loss': 8.40384765625, 'epoch': 0, 'step': 100}
 22%|██▏       | 101/455 [05:20<20:01,  3.39s/it]07/12/2023 20:10:53 - INFO - __main__ - {'train_loss': 8.325514387376238, 'epoch': 0, 'step': 101}
 22%|██▏       | 102/455 [05:23<20:14,  3.44s/it]07/12/2023 20:10:57 - INFO - __main__ - {'train_loss': 8.264418658088236, 'epoch': 0, 'step': 102}
 23%|██▎       | 103/455 [05:27<19:50,  3.38s/it]07/12/2023 20:11:00 - INFO - __main__ - {'train_loss': 8.187651699029127, 'epoch': 0, 'step': 103}
 23%|██▎       | 104/455 [05:30<20:01,  3.42s/it]07/12/2023 20:11:03 - INFO - __main__ - {'train_loss': 8.113055889423077, 'epoch': 0, 'step': 104}
 23%|██▎       | 105/455 [05:34<19:59,  3.43s/it]07/12/2023 20:11:07 - INFO - __main__ - {'train_loss': 8.042224702380953, 'epoch': 0, 'step': 105}
 23%|██▎       | 106/455 [05:37<20:14,  3.48s/it]07/12/2023 20:11:11 - INFO - __main__ - {'train_loss': 7.9715875589622645, 'epoch': 0, 'step': 106}
 24%|██▎       | 107/455 [05:41<20:03,  3.46s/it]07/12/2023 20:11:14 - INFO - __main__ - {'train_loss': 7.9148291471962615, 'epoch': 0, 'step': 107}
 24%|██▎       | 108/455 [05:44<19:44,  3.41s/it]07/12/2023 20:11:17 - INFO - __main__ - {'train_loss': 7.845793547453703, 'epoch': 0, 'step': 108}
 24%|██▍       | 109/455 [05:48<19:52,  3.45s/it]07/12/2023 20:11:21 - INFO - __main__ - {'train_loss': 7.776985378440367, 'epoch': 0, 'step': 109}
 24%|██▍       | 110/455 [05:51<19:28,  3.39s/it]07/12/2023 20:11:24 - INFO - __main__ - {'train_loss': 7.714737215909091, 'epoch': 0, 'step': 110}
 24%|██▍       | 111/455 [05:55<20:29,  3.58s/it]07/12/2023 20:11:28 - INFO - __main__ - {'train_loss': 7.658255912162162, 'epoch': 0, 'step': 111}
 25%|██▍       | 112/455 [05:59<21:11,  3.71s/it]07/12/2023 20:11:32 - INFO - __main__ - {'train_loss': 7.594220842633929, 'epoch': 0, 'step': 112}
 25%|██▍       | 113/455 [06:03<21:28,  3.77s/it]07/12/2023 20:11:36 - INFO - __main__ - {'train_loss': 7.533687085176991, 'epoch': 0, 'step': 113}
 25%|██▌       | 114/455 [06:06<20:51,  3.67s/it]07/12/2023 20:11:39 - INFO - __main__ - {'train_loss': 7.475003426535087, 'epoch': 0, 'step': 114}
 25%|██▌       | 115/455 [06:09<20:07,  3.55s/it]07/12/2023 20:11:43 - INFO - __main__ - {'train_loss': 7.418970788043478, 'epoch': 0, 'step': 115}
 25%|██▌       | 116/455 [06:13<19:51,  3.51s/it]07/12/2023 20:11:46 - INFO - __main__ - {'train_loss': 7.3595602101293105, 'epoch': 0, 'step': 116}
 26%|██▌       | 117/455 [06:16<20:00,  3.55s/it]07/12/2023 20:11:50 - INFO - __main__ - {'train_loss': 7.302467280982906, 'epoch': 0, 'step': 117}
 26%|██▌       | 118/455 [06:20<20:09,  3.59s/it]07/12/2023 20:11:53 - INFO - __main__ - {'train_loss': 7.2505131091101696, 'epoch': 0, 'step': 118}
 26%|██▌       | 119/455 [06:24<20:27,  3.65s/it]07/12/2023 20:11:57 - INFO - __main__ - {'train_loss': 7.203174238445378, 'epoch': 0, 'step': 119}
 26%|██▋       | 120/455 [06:28<20:15,  3.63s/it]07/12/2023 20:12:01 - INFO - __main__ - {'train_loss': 7.147932942708334, 'epoch': 0, 'step': 120}
 27%|██▋       | 121/455 [06:31<19:45,  3.55s/it]07/12/2023 20:12:04 - INFO - __main__ - {'train_loss': 7.097026730371901, 'epoch': 0, 'step': 121}
 27%|██▋       | 122/455 [06:34<19:38,  3.54s/it]07/12/2023 20:12:08 - INFO - __main__ - {'train_loss': 7.041768058401639, 'epoch': 0, 'step': 122}
 27%|██▋       | 123/455 [06:38<20:07,  3.64s/it]07/12/2023 20:12:12 - INFO - __main__ - {'train_loss': 6.989154598577236, 'epoch': 0, 'step': 123}
 27%|██▋       | 124/455 [06:42<19:48,  3.59s/it]07/12/2023 20:12:15 - INFO - __main__ - {'train_loss': 6.942997101814516, 'epoch': 0, 'step': 124}
 27%|██▋       | 125/455 [06:45<19:39,  3.58s/it]07/12/2023 20:12:19 - INFO - __main__ - {'train_loss': 6.901015625, 'epoch': 0, 'step': 125}
 28%|██▊       | 126/455 [06:49<20:18,  3.70s/it]07/12/2023 20:12:23 - INFO - __main__ - {'train_loss': 6.853469122023809, 'epoch': 0, 'step': 126}
 28%|██▊       | 127/455 [06:53<20:02,  3.67s/it]07/12/2023 20:12:26 - INFO - __main__ - {'train_loss': 6.806917445866142, 'epoch': 0, 'step': 127}
 28%|██▊       | 128/455 [06:57<20:00,  3.67s/it]07/12/2023 20:12:30 - INFO - __main__ - {'train_loss': 6.7560882568359375, 'epoch': 0, 'step': 128}
 28%|██▊       | 129/455 [07:00<19:26,  3.58s/it]07/12/2023 20:12:33 - INFO - __main__ - {'train_loss': 6.713284278100775, 'epoch': 0, 'step': 129}
 29%|██▊       | 130/455 [07:03<18:55,  3.49s/it]07/12/2023 20:12:36 - INFO - __main__ - {'train_loss': 6.664212740384615, 'epoch': 0, 'step': 130}
 29%|██▉       | 131/455 [07:07<18:43,  3.47s/it]07/12/2023 20:12:40 - INFO - __main__ - {'train_loss': 6.62115338740458, 'epoch': 0, 'step': 131}
 29%|██▉       | 132/455 [07:10<18:15,  3.39s/it]07/12/2023 20:12:43 - INFO - __main__ - {'train_loss': 6.576171875, 'epoch': 0, 'step': 132}
 29%|██▉       | 133/455 [07:13<18:14,  3.40s/it]07/12/2023 20:12:46 - INFO - __main__ - {'train_loss': 6.528606672932331, 'epoch': 0, 'step': 133}
 29%|██▉       | 134/455 [07:17<18:17,  3.42s/it]07/12/2023 20:12:50 - INFO - __main__ - {'train_loss': 6.482859141791045, 'epoch': 0, 'step': 134}
 30%|██▉       | 135/455 [07:20<17:43,  3.32s/it]07/12/2023 20:12:53 - INFO - __main__ - {'train_loss': 6.435966435185185, 'epoch': 0, 'step': 135}
 30%|██▉       | 136/455 [07:23<17:10,  3.23s/it]07/12/2023 20:12:56 - INFO - __main__ - {'train_loss': 6.393870634191177, 'epoch': 0, 'step': 136}
 30%|███       | 137/455 [07:26<17:05,  3.22s/it]07/12/2023 20:12:59 - INFO - __main__ - {'train_loss': 6.357008439781022, 'epoch': 0, 'step': 137}
 30%|███       | 138/455 [07:29<16:45,  3.17s/it]07/12/2023 20:13:02 - INFO - __main__ - {'train_loss': 6.31544384057971, 'epoch': 0, 'step': 138}
 31%|███       | 139/455 [07:32<16:07,  3.06s/it]07/12/2023 20:13:05 - INFO - __main__ - {'train_loss': 6.272594424460432, 'epoch': 0, 'step': 139}
 31%|███       | 140/455 [07:35<15:39,  2.98s/it]07/12/2023 20:13:08 - INFO - __main__ - {'train_loss': 6.234542410714286, 'epoch': 0, 'step': 140}
 31%|███       | 141/455 [07:38<15:24,  2.94s/it]07/12/2023 20:13:11 - INFO - __main__ - {'train_loss': 6.193276263297872, 'epoch': 0, 'step': 141}
 31%|███       | 142/455 [07:40<15:19,  2.94s/it]07/12/2023 20:13:14 - INFO - __main__ - {'train_loss': 6.15019462477993, 'epoch': 0, 'step': 142}
 31%|███▏      | 143/455 [07:43<14:47,  2.85s/it]07/12/2023 20:13:16 - INFO - __main__ - {'train_loss': 6.1109559385926575, 'epoch': 0, 'step': 143}
 32%|███▏      | 144/455 [07:46<14:16,  2.75s/it]07/12/2023 20:13:19 - INFO - __main__ - {'train_loss': 6.070973714192708, 'epoch': 0, 'step': 144}
 32%|███▏      | 145/455 [07:48<14:20,  2.78s/it]07/12/2023 20:13:22 - INFO - __main__ - {'train_loss': 6.033172817887931, 'epoch': 0, 'step': 145}
 32%|███▏      | 146/455 [07:51<14:21,  2.79s/it]07/12/2023 20:13:25 - INFO - __main__ - {'train_loss': 5.998806052011986, 'epoch': 0, 'step': 146}
 32%|███▏      | 147/455 [07:54<14:54,  2.91s/it]07/12/2023 20:13:28 - INFO - __main__ - {'train_loss': 5.959466012967687, 'epoch': 0, 'step': 147}
 33%|███▎      | 148/455 [07:57<14:05,  2.76s/it]07/12/2023 20:13:30 - INFO - __main__ - {'train_loss': 5.922353383657095, 'epoch': 0, 'step': 148}
 33%|███▎      | 149/455 [08:00<14:13,  2.79s/it]07/12/2023 20:13:33 - INFO - __main__ - {'train_loss': 5.886197698196309, 'epoch': 0, 'step': 149}
 33%|███▎      | 150/455 [08:02<13:52,  2.73s/it]07/12/2023 20:13:36 - INFO - __main__ - {'train_loss': 5.850888671875, 'epoch': 0, 'step': 150}
 33%|███▎      | 151/455 [08:05<13:32,  2.67s/it]07/12/2023 20:13:38 - INFO - __main__ - {'train_loss': 5.815219500206954, 'epoch': 0, 'step': 151}
 33%|███▎      | 152/455 [08:08<13:29,  2.67s/it]07/12/2023 20:13:41 - INFO - __main__ - {'train_loss': 5.782846551192434, 'epoch': 0, 'step': 152}
 34%|███▎      | 153/455 [08:11<14:19,  2.85s/it]07/12/2023 20:13:44 - INFO - __main__ - {'train_loss': 5.748152190563726, 'epoch': 0, 'step': 153}
 34%|███▍      | 154/455 [08:14<14:47,  2.95s/it]07/12/2023 20:13:47 - INFO - __main__ - {'train_loss': 5.7117840655438314, 'epoch': 0, 'step': 154}
 34%|███▍      | 155/455 [08:17<15:04,  3.01s/it]07/12/2023 20:13:50 - INFO - __main__ - {'train_loss': 5.676275831653226, 'epoch': 0, 'step': 155}
 34%|███▍      | 156/455 [08:20<15:26,  3.10s/it]07/12/2023 20:13:54 - INFO - __main__ - {'train_loss': 5.644296499399038, 'epoch': 0, 'step': 156}
 35%|███▍      | 157/455 [08:24<15:39,  3.15s/it]07/12/2023 20:13:57 - INFO - __main__ - {'train_loss': 5.612973352906051, 'epoch': 0, 'step': 157}
 35%|███▍      | 158/455 [08:27<15:43,  3.18s/it]07/12/2023 20:14:00 - INFO - __main__ - {'train_loss': 5.578671998615507, 'epoch': 0, 'step': 158}
 35%|███▍      | 159/455 [08:30<15:38,  3.17s/it]07/12/2023 20:14:03 - INFO - __main__ - {'train_loss': 5.545821663718553, 'epoch': 0, 'step': 159}
 35%|███▌      | 160/455 [08:33<15:34,  3.17s/it]07/12/2023 20:14:07 - INFO - __main__ - {'train_loss': 5.517239379882812, 'epoch': 0, 'step': 160}
 35%|███▌      | 161/455 [08:37<15:54,  3.25s/it]07/12/2023 20:14:10 - INFO - __main__ - {'train_loss': 5.485906565411491, 'epoch': 0, 'step': 161}
 36%|███▌      | 162/455 [08:40<16:00,  3.28s/it]07/12/2023 20:14:13 - INFO - __main__ - {'train_loss': 5.457154827353395, 'epoch': 0, 'step': 162}
 36%|███▌      | 163/455 [08:43<16:05,  3.31s/it]07/12/2023 20:14:17 - INFO - __main__ - {'train_loss': 5.425880104486196, 'epoch': 0, 'step': 163}
 36%|███▌      | 164/455 [08:47<16:00,  3.30s/it]07/12/2023 20:14:20 - INFO - __main__ - {'train_loss': 5.3943913157393295, 'epoch': 0, 'step': 164}
 36%|███▋      | 165/455 [08:50<16:01,  3.32s/it]07/12/2023 20:14:23 - INFO - __main__ - {'train_loss': 5.364207504734848, 'epoch': 0, 'step': 165}
 36%|███▋      | 166/455 [08:53<16:02,  3.33s/it]07/12/2023 20:14:27 - INFO - __main__ - {'train_loss': 5.337164086031627, 'epoch': 0, 'step': 166}
 37%|███▋      | 167/455 [08:57<16:30,  3.44s/it]07/12/2023 20:14:30 - INFO - __main__ - {'train_loss': 5.30818733626497, 'epoch': 0, 'step': 167}
 37%|███▋      | 168/455 [09:00<16:08,  3.37s/it]07/12/2023 20:14:34 - INFO - __main__ - {'train_loss': 5.280287969680059, 'epoch': 0, 'step': 168}
 37%|███▋      | 169/455 [09:04<16:06,  3.38s/it]07/12/2023 20:14:37 - INFO - __main__ - {'train_loss': 5.251667090421598, 'epoch': 0, 'step': 169}
 37%|███▋      | 170/455 [09:07<15:59,  3.37s/it]07/12/2023 20:14:40 - INFO - __main__ - {'train_loss': 5.224037798713235, 'epoch': 0, 'step': 170}
 38%|███▊      | 171/455 [09:11<16:09,  3.41s/it]07/12/2023 20:14:44 - INFO - __main__ - {'train_loss': 5.195292511878655, 'epoch': 0, 'step': 171}
 38%|███▊      | 172/455 [09:14<16:19,  3.46s/it]07/12/2023 20:14:47 - INFO - __main__ - {'train_loss': 5.171173805414244, 'epoch': 0, 'step': 172}
 38%|███▊      | 173/455 [09:18<16:04,  3.42s/it]07/12/2023 20:14:51 - INFO - __main__ - {'train_loss': 5.144624390354046, 'epoch': 0, 'step': 173}
 38%|███▊      | 174/455 [09:21<16:10,  3.45s/it]07/12/2023 20:14:54 - INFO - __main__ - {'train_loss': 5.122690485811781, 'epoch': 0, 'step': 174}
 38%|███▊      | 175/455 [09:24<16:03,  3.44s/it]07/12/2023 20:14:58 - INFO - __main__ - {'train_loss': 5.09560546875, 'epoch': 0, 'step': 175}
 39%|███▊      | 176/455 [09:28<16:03,  3.45s/it]07/12/2023 20:15:01 - INFO - __main__ - {'train_loss': 5.069138960404829, 'epoch': 0, 'step': 176}
 39%|███▉      | 177/455 [09:31<15:55,  3.44s/it]07/12/2023 20:15:05 - INFO - __main__ - {'train_loss': 5.042772885769774, 'epoch': 0, 'step': 177}
 39%|███▉      | 178/455 [09:35<15:30,  3.36s/it]07/12/2023 20:15:08 - INFO - __main__ - {'train_loss': 5.016549442591292, 'epoch': 0, 'step': 178}
 39%|███▉      | 179/455 [09:38<15:18,  3.33s/it]07/12/2023 20:15:11 - INFO - __main__ - {'train_loss': 4.994372490398045, 'epoch': 0, 'step': 179}
 40%|███▉      | 180/455 [09:42<15:51,  3.46s/it]07/12/2023 20:15:15 - INFO - __main__ - {'train_loss': 4.970836046006944, 'epoch': 0, 'step': 180}
 40%|███▉      | 181/455 [09:45<15:10,  3.32s/it]07/12/2023 20:15:18 - INFO - __main__ - {'train_loss': 4.94695539105663, 'epoch': 0, 'step': 181}
 40%|████      | 182/455 [09:47<14:11,  3.12s/it]07/12/2023 20:15:20 - INFO - __main__ - {'train_loss': 4.923766419127747, 'epoch': 0, 'step': 182}
 40%|████      | 183/455 [09:50<13:47,  3.04s/it]07/12/2023 20:15:23 - INFO - __main__ - {'train_loss': 4.8992833205259565, 'epoch': 0, 'step': 183}
 40%|████      | 184/455 [09:53<13:05,  2.90s/it]07/12/2023 20:15:26 - INFO - __main__ - {'train_loss': 4.873179560122282, 'epoch': 0, 'step': 184}
 41%|████      | 185/455 [09:55<12:56,  2.88s/it]07/12/2023 20:15:29 - INFO - __main__ - {'train_loss': 4.850089738175676, 'epoch': 0, 'step': 185}
 41%|████      | 186/455 [09:58<12:42,  2.83s/it]07/12/2023 20:15:31 - INFO - __main__ - {'train_loss': 4.825589087701613, 'epoch': 0, 'step': 186}
 41%|████      | 187/455 [10:01<12:31,  2.81s/it]07/12/2023 20:15:34 - INFO - __main__ - {'train_loss': 4.8019144802807485, 'epoch': 0, 'step': 187}
 41%|████▏     | 188/455 [10:04<12:34,  2.83s/it]07/12/2023 20:15:37 - INFO - __main__ - {'train_loss': 4.777052859042553, 'epoch': 0, 'step': 188}
 42%|████▏     | 189/455 [10:06<12:03,  2.72s/it]07/12/2023 20:15:39 - INFO - __main__ - {'train_loss': 4.752031921709656, 'epoch': 0, 'step': 189}
 42%|████▏     | 190/455 [10:09<12:08,  2.75s/it]07/12/2023 20:15:42 - INFO - __main__ - {'train_loss': 4.731359220805921, 'epoch': 0, 'step': 190}
 42%|████▏     | 191/455 [10:12<12:11,  2.77s/it]07/12/2023 20:15:45 - INFO - __main__ - {'train_loss': 4.707789236338351, 'epoch': 0, 'step': 191}
 42%|████▏     | 192/455 [10:15<12:06,  2.76s/it]07/12/2023 20:15:48 - INFO - __main__ - {'train_loss': 4.685293833414714, 'epoch': 0, 'step': 192}
 42%|████▏     | 193/455 [10:17<12:10,  2.79s/it]07/12/2023 20:15:51 - INFO - __main__ - {'train_loss': 4.669396909407383, 'epoch': 0, 'step': 193}
 43%|████▎     | 194/455 [10:20<12:03,  2.77s/it]07/12/2023 20:15:53 - INFO - __main__ - {'train_loss': 4.645695322567654, 'epoch': 0, 'step': 194}
 43%|████▎     | 195/455 [10:23<12:09,  2.80s/it]07/12/2023 20:15:56 - INFO - __main__ - {'train_loss': 4.62407476963141, 'epoch': 0, 'step': 195}
 43%|████▎     | 196/455 [10:26<12:04,  2.80s/it]07/12/2023 20:15:59 - INFO - __main__ - {'train_loss': 4.602007184709821, 'epoch': 0, 'step': 196}
 43%|████▎     | 197/455 [10:29<12:00,  2.79s/it]07/12/2023 20:16:02 - INFO - __main__ - {'train_loss': 4.581006355092005, 'epoch': 0, 'step': 197}
 44%|████▎     | 198/455 [10:32<12:06,  2.83s/it]07/12/2023 20:16:05 - INFO - __main__ - {'train_loss': 4.558673897174874, 'epoch': 0, 'step': 198}
 44%|████▎     | 199/455 [10:34<12:00,  2.82s/it]07/12/2023 20:16:08 - INFO - __main__ - {'train_loss': 4.537140045932789, 'epoch': 0, 'step': 199}
 44%|████▍     | 200/455 [10:37<12:19,  2.90s/it]07/12/2023 20:16:11 - INFO - __main__ - {'train_loss': 4.515465087890625, 'epoch': 0, 'step': 200}
 44%|████▍     | 201/455 [10:40<11:59,  2.83s/it]07/12/2023 20:16:13 - INFO - __main__ - {'train_loss': 4.494059244791667, 'epoch': 0, 'step': 201}
 44%|████▍     | 202/455 [10:43<11:58,  2.84s/it]07/12/2023 20:16:16 - INFO - __main__ - {'train_loss': 4.475620987391708, 'epoch': 0, 'step': 202}
 45%|████▍     | 203/455 [10:46<12:15,  2.92s/it]07/12/2023 20:16:19 - INFO - __main__ - {'train_loss': 4.455536339670567, 'epoch': 0, 'step': 203}
 45%|████▍     | 204/455 [10:49<12:43,  3.04s/it]07/12/2023 20:16:23 - INFO - __main__ - {'train_loss': 4.434403961780024, 'epoch': 0, 'step': 204}
 45%|████▌     | 205/455 [10:53<13:23,  3.21s/it]07/12/2023 20:16:26 - INFO - __main__ - {'train_loss': 4.413939834222561, 'epoch': 0, 'step': 205}
 45%|████▌     | 206/455 [10:57<13:49,  3.33s/it]07/12/2023 20:16:30 - INFO - __main__ - {'train_loss': 4.395926169978762, 'epoch': 0, 'step': 206}
 45%|████▌     | 207/455 [11:00<13:44,  3.33s/it]07/12/2023 20:16:33 - INFO - __main__ - {'train_loss': 4.375897541138285, 'epoch': 0, 'step': 207}
 46%|████▌     | 208/455 [11:03<13:40,  3.32s/it]07/12/2023 20:16:36 - INFO - __main__ - {'train_loss': 4.359592144305889, 'epoch': 0, 'step': 208}
 46%|████▌     | 209/455 [11:07<13:30,  3.30s/it]07/12/2023 20:16:40 - INFO - __main__ - {'train_loss': 4.3399243514503585, 'epoch': 0, 'step': 209}
 46%|████▌     | 210/455 [11:10<13:44,  3.36s/it]07/12/2023 20:16:43 - INFO - __main__ - {'train_loss': 4.320746140252976, 'epoch': 0, 'step': 210}
 46%|████▋     | 211/455 [11:13<13:47,  3.39s/it]07/12/2023 20:16:47 - INFO - __main__ - {'train_loss': 4.302527260441351, 'epoch': 0, 'step': 211}
 47%|████▋     | 212/455 [11:17<14:04,  3.47s/it]07/12/2023 20:16:50 - INFO - __main__ - {'train_loss': 4.283351682267099, 'epoch': 0, 'step': 212}
 47%|████▋     | 213/455 [11:21<14:02,  3.48s/it]07/12/2023 20:16:54 - INFO - __main__ - {'train_loss': 4.267790172021714, 'epoch': 0, 'step': 213}
 47%|████▋     | 214/455 [11:24<14:15,  3.55s/it]07/12/2023 20:16:58 - INFO - __main__ - {'train_loss': 4.249197986638435, 'epoch': 0, 'step': 214}
 47%|████▋     | 215/455 [11:28<14:18,  3.58s/it]07/12/2023 20:17:01 - INFO - __main__ - {'train_loss': 4.23058343931686, 'epoch': 0, 'step': 215}
 47%|████▋     | 216/455 [11:31<14:00,  3.52s/it]07/12/2023 20:17:05 - INFO - __main__ - {'train_loss': 4.217073793764468, 'epoch': 0, 'step': 216}
 48%|████▊     | 217/455 [11:35<14:01,  3.54s/it]07/12/2023 20:17:08 - INFO - __main__ - {'train_loss': 4.198990360383065, 'epoch': 0, 'step': 217}
 48%|████▊     | 218/455 [11:38<13:50,  3.50s/it]07/12/2023 20:17:12 - INFO - __main__ - {'train_loss': 4.180848847835436, 'epoch': 0, 'step': 218}
 48%|████▊     | 219/455 [11:42<13:37,  3.46s/it]07/12/2023 20:17:15 - INFO - __main__ - {'train_loss': 4.163809440996005, 'epoch': 0, 'step': 219}
 48%|████▊     | 220/455 [11:45<13:22,  3.42s/it]07/12/2023 20:17:18 - INFO - __main__ - {'train_loss': 4.145930619673296, 'epoch': 0, 'step': 220}
 49%|████▊     | 221/455 [11:48<13:10,  3.38s/it]07/12/2023 20:17:22 - INFO - __main__ - {'train_loss': 4.129981131575226, 'epoch': 0, 'step': 221}
 49%|████▉     | 222/455 [11:52<13:01,  3.35s/it]07/12/2023 20:17:25 - INFO - __main__ - {'train_loss': 4.113445110149212, 'epoch': 0, 'step': 222}
 49%|████▉     | 223/455 [11:55<12:45,  3.30s/it]07/12/2023 20:17:28 - INFO - __main__ - {'train_loss': 4.0959625928391254, 'epoch': 0, 'step': 223}
 49%|████▉     | 224/455 [11:58<12:30,  3.25s/it]07/12/2023 20:17:31 - INFO - __main__ - {'train_loss': 4.0780214582170755, 'epoch': 0, 'step': 224}
 49%|████▉     | 225/455 [12:01<11:42,  3.06s/it]07/12/2023 20:17:34 - INFO - __main__ - {'train_loss': 4.062587890625, 'epoch': 0, 'step': 225}
 50%|████▉     | 226/455 [12:03<11:26,  3.00s/it]07/12/2023 20:17:37 - INFO - __main__ - {'train_loss': 4.045030982093474, 'epoch': 0, 'step': 226}
 50%|████▉     | 227/455 [12:06<11:14,  2.96s/it]07/12/2023 20:17:40 - INFO - __main__ - {'train_loss': 4.028312783934472, 'epoch': 0, 'step': 227}
 50%|█████     | 228/455 [12:09<10:36,  2.80s/it]07/12/2023 20:17:42 - INFO - __main__ - {'train_loss': 4.012606436746163, 'epoch': 0, 'step': 228}
 50%|█████     | 229/455 [12:12<10:39,  2.83s/it]07/12/2023 20:17:45 - INFO - __main__ - {'train_loss': 3.9971225521970526, 'epoch': 0, 'step': 229}
 51%|█████     | 230/455 [12:14<10:26,  2.79s/it]07/12/2023 20:17:48 - INFO - __main__ - {'train_loss': 3.9812807829483696, 'epoch': 0, 'step': 230}
 51%|█████     | 231/455 [12:17<10:22,  2.78s/it]07/12/2023 20:17:50 - INFO - __main__ - {'train_loss': 3.966058111810065, 'epoch': 0, 'step': 231}
 51%|█████     | 232/455 [12:20<10:11,  2.74s/it]07/12/2023 20:17:53 - INFO - __main__ - {'train_loss': 3.949884875067349, 'epoch': 0, 'step': 232}
 51%|█████     | 233/455 [12:23<10:13,  2.76s/it]07/12/2023 20:17:56 - INFO - __main__ - {'train_loss': 3.9350282071486054, 'epoch': 0, 'step': 233}
 51%|█████▏    | 234/455 [12:25<09:58,  2.71s/it]07/12/2023 20:17:58 - INFO - __main__ - {'train_loss': 3.9194972372462606, 'epoch': 0, 'step': 234}
 52%|█████▏    | 235/455 [12:28<09:50,  2.68s/it]07/12/2023 20:18:01 - INFO - __main__ - {'train_loss': 3.9044724484707447, 'epoch': 0, 'step': 235}
 52%|█████▏    | 236/455 [12:31<10:01,  2.74s/it]07/12/2023 20:18:04 - INFO - __main__ - {'train_loss': 3.8901625811043434, 'epoch': 0, 'step': 236}
 52%|█████▏    | 237/455 [12:33<10:03,  2.77s/it]07/12/2023 20:18:07 - INFO - __main__ - {'train_loss': 3.8759405079773206, 'epoch': 0, 'step': 237}
 52%|█████▏    | 238/455 [12:36<10:08,  2.81s/it]07/12/2023 20:18:10 - INFO - __main__ - {'train_loss': 3.8604962004332983, 'epoch': 0, 'step': 238}
 53%|█████▎    | 239/455 [12:39<09:51,  2.74s/it]07/12/2023 20:18:12 - INFO - __main__ - {'train_loss': 3.8455856514775104, 'epoch': 0, 'step': 239}
 53%|█████▎    | 240/455 [12:42<10:06,  2.82s/it]07/12/2023 20:18:15 - INFO - __main__ - {'train_loss': 3.8315399169921873, 'epoch': 0, 'step': 240}
 53%|█████▎    | 241/455 [12:45<10:26,  2.93s/it]07/12/2023 20:18:18 - INFO - __main__ - {'train_loss': 3.817140697938278, 'epoch': 0, 'step': 241}
 53%|█████▎    | 242/455 [12:49<10:55,  3.08s/it]07/12/2023 20:18:22 - INFO - __main__ - {'train_loss': 3.80314295745093, 'epoch': 0, 'step': 242}
 53%|█████▎    | 243/455 [12:52<11:13,  3.18s/it]07/12/2023 20:18:25 - INFO - __main__ - {'train_loss': 3.788311993634259, 'epoch': 0, 'step': 243}
 54%|█████▎    | 244/455 [12:55<11:14,  3.20s/it]07/12/2023 20:18:28 - INFO - __main__ - {'train_loss': 3.7736666319800203, 'epoch': 0, 'step': 244}
 54%|█████▍    | 245/455 [12:59<11:40,  3.33s/it]07/12/2023 20:18:32 - INFO - __main__ - {'train_loss': 3.760527941645408, 'epoch': 0, 'step': 245}
 54%|█████▍    | 246/455 [13:02<11:38,  3.34s/it]07/12/2023 20:18:35 - INFO - __main__ - {'train_loss': 3.747242004890752, 'epoch': 0, 'step': 246}
 54%|█████▍    | 247/455 [13:06<11:41,  3.37s/it]07/12/2023 20:18:39 - INFO - __main__ - {'train_loss': 3.73610375284666, 'epoch': 0, 'step': 247}
 55%|█████▍    | 248/455 [13:09<11:34,  3.36s/it]07/12/2023 20:18:42 - INFO - __main__ - {'train_loss': 3.7214837843371975, 'epoch': 0, 'step': 248}
 55%|█████▍    | 249/455 [13:12<11:32,  3.36s/it]07/12/2023 20:18:46 - INFO - __main__ - {'train_loss': 3.7080989975527108, 'epoch': 0, 'step': 249}
 55%|█████▍    | 250/455 [13:16<11:40,  3.42s/it]07/12/2023 20:18:49 - INFO - __main__ - {'train_loss': 3.6943916015625, 'epoch': 0, 'step': 250}
 55%|█████▌    | 251/455 [13:19<11:45,  3.46s/it]07/12/2023 20:18:53 - INFO - __main__ - {'train_loss': 3.6799686021538847, 'epoch': 0, 'step': 251}
 55%|█████▌    | 252/455 [13:23<11:48,  3.49s/it]07/12/2023 20:18:56 - INFO - __main__ - {'train_loss': 3.666628882998512, 'epoch': 0, 'step': 252}
 56%|█████▌    | 253/455 [13:27<11:47,  3.50s/it]07/12/2023 20:19:00 - INFO - __main__ - {'train_loss': 3.6536184921566206, 'epoch': 0, 'step': 253}
 56%|█████▌    | 254/455 [13:30<11:33,  3.45s/it]07/12/2023 20:19:03 - INFO - __main__ - {'train_loss': 3.6407259242741143, 'epoch': 0, 'step': 254}
 56%|█████▌    | 255/455 [13:34<12:00,  3.60s/it]07/12/2023 20:19:07 - INFO - __main__ - {'train_loss': 3.6319632735906864, 'epoch': 0, 'step': 255}
 56%|█████▋    | 256/455 [13:37<11:40,  3.52s/it]07/12/2023 20:19:10 - INFO - __main__ - {'train_loss': 3.6204004287719727, 'epoch': 0, 'step': 256}
 56%|█████▋    | 257/455 [13:41<11:28,  3.48s/it]07/12/2023 20:19:14 - INFO - __main__ - {'train_loss': 3.6065906569187742, 'epoch': 0, 'step': 257}
 57%|█████▋    | 258/455 [13:44<11:06,  3.39s/it]07/12/2023 20:19:17 - INFO - __main__ - {'train_loss': 3.593467061833818, 'epoch': 0, 'step': 258}
 57%|█████▋    | 259/455 [13:47<11:06,  3.40s/it]07/12/2023 20:19:20 - INFO - __main__ - {'train_loss': 3.580874645571911, 'epoch': 0, 'step': 259}
 57%|█████▋    | 260/455 [13:50<10:54,  3.36s/it]07/12/2023 20:19:24 - INFO - __main__ - {'train_loss': 3.5685443584735577, 'epoch': 0, 'step': 260}
 57%|█████▋    | 261/455 [13:54<10:45,  3.33s/it]07/12/2023 20:19:27 - INFO - __main__ - {'train_loss': 3.5576405726173372, 'epoch': 0, 'step': 261}
 58%|█████▊    | 262/455 [13:57<10:36,  3.30s/it]07/12/2023 20:19:30 - INFO - __main__ - {'train_loss': 3.5448258960520036, 'epoch': 0, 'step': 262}
 58%|█████▊    | 263/455 [14:00<10:19,  3.23s/it]07/12/2023 20:19:33 - INFO - __main__ - {'train_loss': 3.5326247994890685, 'epoch': 0, 'step': 263}
 58%|█████▊    | 264/455 [14:03<10:15,  3.22s/it]07/12/2023 20:19:36 - INFO - __main__ - {'train_loss': 3.519387909860322, 'epoch': 0, 'step': 264}
 58%|█████▊    | 265/455 [14:06<09:28,  2.99s/it]07/12/2023 20:19:39 - INFO - __main__ - {'train_loss': 3.50695109817217, 'epoch': 0, 'step': 265}
 58%|█████▊    | 266/455 [14:08<09:05,  2.89s/it]07/12/2023 20:19:41 - INFO - __main__ - {'train_loss': 3.4967922124647557, 'epoch': 0, 'step': 266}
 59%|█████▊    | 267/455 [14:11<08:56,  2.85s/it]07/12/2023 20:19:44 - INFO - __main__ - {'train_loss': 3.485282983672753, 'epoch': 0, 'step': 267}
 59%|█████▉    | 268/455 [14:14<08:52,  2.85s/it]07/12/2023 20:19:47 - INFO - __main__ - {'train_loss': 3.474726890450093, 'epoch': 0, 'step': 268}
 59%|█████▉    | 269/455 [14:17<08:45,  2.82s/it]07/12/2023 20:19:50 - INFO - __main__ - {'train_loss': 3.4632037421584574, 'epoch': 0, 'step': 269}
 59%|█████▉    | 270/455 [14:19<08:37,  2.80s/it]07/12/2023 20:19:53 - INFO - __main__ - {'train_loss': 3.451903392650463, 'epoch': 0, 'step': 270}
 60%|█████▉    | 271/455 [14:22<08:35,  2.80s/it]07/12/2023 20:19:55 - INFO - __main__ - {'train_loss': 3.4396378070226015, 'epoch': 0, 'step': 271}
 60%|█████▉    | 272/455 [14:25<08:32,  2.80s/it]07/12/2023 20:19:58 - INFO - __main__ - {'train_loss': 3.428327672621783, 'epoch': 0, 'step': 272}
 60%|██████    | 273/455 [14:28<08:16,  2.73s/it]07/12/2023 20:20:01 - INFO - __main__ - {'train_loss': 3.4164422003777473, 'epoch': 0, 'step': 273}
 60%|██████    | 274/455 [14:30<08:10,  2.71s/it]07/12/2023 20:20:03 - INFO - __main__ - {'train_loss': 3.406881736142792, 'epoch': 0, 'step': 274}
 60%|██████    | 275/455 [14:33<08:24,  2.80s/it]07/12/2023 20:20:06 - INFO - __main__ - {'train_loss': 3.3961194957386365, 'epoch': 0, 'step': 275}
 61%|██████    | 276/455 [14:36<08:10,  2.74s/it]07/12/2023 20:20:09 - INFO - __main__ - {'train_loss': 3.3841260827105977, 'epoch': 0, 'step': 276}
 61%|██████    | 277/455 [14:38<08:03,  2.72s/it]07/12/2023 20:20:12 - INFO - __main__ - {'train_loss': 3.372550661383123, 'epoch': 0, 'step': 277}
 61%|██████    | 278/455 [14:41<08:14,  2.80s/it]07/12/2023 20:20:15 - INFO - __main__ - {'train_loss': 3.3618032331946943, 'epoch': 0, 'step': 278}
 61%|██████▏   | 279/455 [14:44<08:16,  2.82s/it]07/12/2023 20:20:18 - INFO - __main__ - {'train_loss': 3.3501160324260755, 'epoch': 0, 'step': 279}
 62%|██████▏   | 280/455 [14:48<08:38,  2.96s/it]07/12/2023 20:20:21 - INFO - __main__ - {'train_loss': 3.339546421595982, 'epoch': 0, 'step': 280}
 62%|██████▏   | 281/455 [14:51<08:59,  3.10s/it]07/12/2023 20:20:24 - INFO - __main__ - {'train_loss': 3.3296775953625444, 'epoch': 0, 'step': 281}
 62%|██████▏   | 282/455 [14:55<09:14,  3.20s/it]07/12/2023 20:20:28 - INFO - __main__ - {'train_loss': 3.3184727878435285, 'epoch': 0, 'step': 282}
 62%|██████▏   | 283/455 [14:58<09:18,  3.25s/it]07/12/2023 20:20:31 - INFO - __main__ - {'train_loss': 3.3084997170384276, 'epoch': 0, 'step': 283}
 62%|██████▏   | 284/455 [15:01<09:33,  3.35s/it]07/12/2023 20:20:35 - INFO - __main__ - {'train_loss': 3.2972936495928695, 'epoch': 0, 'step': 284}
 63%|██████▎   | 285/455 [15:05<09:18,  3.28s/it]07/12/2023 20:20:38 - INFO - __main__ - {'train_loss': 3.287108518366228, 'epoch': 0, 'step': 285}
 63%|██████▎   | 286/455 [15:08<09:28,  3.36s/it]07/12/2023 20:20:41 - INFO - __main__ - {'train_loss': 3.2766736437390733, 'epoch': 0, 'step': 286}
 63%|██████▎   | 287/455 [15:12<09:25,  3.37s/it]07/12/2023 20:20:45 - INFO - __main__ - {'train_loss': 3.268727372332317, 'epoch': 0, 'step': 287}
 63%|██████▎   | 288/455 [15:15<09:22,  3.37s/it]07/12/2023 20:20:48 - INFO - __main__ - {'train_loss': 3.2579744127061634, 'epoch': 0, 'step': 288}
 64%|██████▎   | 289/455 [15:18<09:19,  3.37s/it]07/12/2023 20:20:51 - INFO - __main__ - {'train_loss': 3.2475053727833045, 'epoch': 0, 'step': 289}
 64%|██████▎   | 290/455 [15:22<09:17,  3.38s/it]07/12/2023 20:20:55 - INFO - __main__ - {'train_loss': 3.2379301926185344, 'epoch': 0, 'step': 290}
 64%|██████▍   | 291/455 [15:25<09:13,  3.37s/it]07/12/2023 20:20:58 - INFO - __main__ - {'train_loss': 3.2271976012134878, 'epoch': 0, 'step': 291}
 64%|██████▍   | 292/455 [15:28<09:07,  3.36s/it]07/12/2023 20:21:02 - INFO - __main__ - {'train_loss': 3.2171890049764555, 'epoch': 0, 'step': 292}
 64%|██████▍   | 293/455 [15:32<09:12,  3.41s/it]07/12/2023 20:21:05 - INFO - __main__ - {'train_loss': 3.209248513491894, 'epoch': 0, 'step': 293}
 65%|██████▍   | 294/455 [15:35<09:11,  3.43s/it]07/12/2023 20:21:09 - INFO - __main__ - {'train_loss': 3.198910667782738, 'epoch': 0, 'step': 294}
 65%|██████▍   | 295/455 [15:39<09:09,  3.44s/it]07/12/2023 20:21:12 - INFO - __main__ - {'train_loss': 3.1896227820444913, 'epoch': 0, 'step': 295}
 65%|██████▌   | 296/455 [15:42<08:54,  3.36s/it]07/12/2023 20:21:15 - INFO - __main__ - {'train_loss': 3.1798829774598816, 'epoch': 0, 'step': 296}
 65%|██████▌   | 297/455 [15:46<09:01,  3.43s/it]07/12/2023 20:21:19 - INFO - __main__ - {'train_loss': 3.16956593934133, 'epoch': 0, 'step': 297}
 65%|██████▌   | 298/455 [15:49<08:47,  3.36s/it]07/12/2023 20:21:22 - INFO - __main__ - {'train_loss': 3.1602996211723995, 'epoch': 0, 'step': 298}
 66%|██████▌   | 299/455 [15:52<08:40,  3.33s/it]07/12/2023 20:21:25 - INFO - __main__ - {'train_loss': 3.1501775122805182, 'epoch': 0, 'step': 299}
 66%|██████▌   | 300/455 [15:55<08:24,  3.26s/it]07/12/2023 20:21:28 - INFO - __main__ - {'train_loss': 3.1402433268229166, 'epoch': 0, 'step': 300}
 66%|██████▌   | 301/455 [15:58<08:15,  3.22s/it]07/12/2023 20:21:31 - INFO - __main__ - {'train_loss': 3.129917689732143, 'epoch': 0, 'step': 301}
 66%|██████▋   | 302/455 [16:02<08:23,  3.29s/it]07/12/2023 20:21:35 - INFO - __main__ - {'train_loss': 3.119970864807533, 'epoch': 0, 'step': 302}
 67%|██████▋   | 303/455 [16:05<08:33,  3.38s/it]07/12/2023 20:21:38 - INFO - __main__ - {'train_loss': 3.1102959661200495, 'epoch': 0, 'step': 303}
 67%|██████▋   | 304/455 [16:09<08:29,  3.37s/it]07/12/2023 20:21:42 - INFO - __main__ - {'train_loss': 3.1009513453433386, 'epoch': 0, 'step': 304}
 67%|██████▋   | 305/455 [16:11<07:45,  3.10s/it]07/12/2023 20:21:44 - INFO - __main__ - {'train_loss': 3.0908759445440572, 'epoch': 0, 'step': 305}
 67%|██████▋   | 306/455 [16:14<07:42,  3.10s/it]07/12/2023 20:21:47 - INFO - __main__ - {'train_loss': 3.08172607421875, 'epoch': 0, 'step': 306}
 67%|██████▋   | 307/455 [16:17<07:30,  3.04s/it]07/12/2023 20:21:50 - INFO - __main__ - {'train_loss': 3.073698261273412, 'epoch': 0, 'step': 307}
 68%|██████▊   | 308/455 [16:20<07:33,  3.08s/it]07/12/2023 20:21:54 - INFO - __main__ - {'train_loss': 3.06543087649655, 'epoch': 0, 'step': 308}
 68%|██████▊   | 309/455 [16:24<07:42,  3.17s/it]07/12/2023 20:21:57 - INFO - __main__ - {'train_loss': 3.05724228547229, 'epoch': 0, 'step': 309}
 68%|██████▊   | 310/455 [16:27<07:45,  3.21s/it]07/12/2023 20:22:00 - INFO - __main__ - {'train_loss': 3.0488104051159275, 'epoch': 0, 'step': 310}
 68%|██████▊   | 311/455 [16:30<07:46,  3.24s/it]07/12/2023 20:22:03 - INFO - __main__ - {'train_loss': 3.0395598089579985, 'epoch': 0, 'step': 311}
 69%|██████▊   | 312/455 [16:34<07:44,  3.25s/it]07/12/2023 20:22:07 - INFO - __main__ - {'train_loss': 3.031683114858774, 'epoch': 0, 'step': 312}
 69%|██████▉   | 313/455 [16:37<07:38,  3.23s/it]07/12/2023 20:22:10 - INFO - __main__ - {'train_loss': 3.023507310178714, 'epoch': 0, 'step': 313}
 69%|██████▉   | 314/455 [16:40<07:27,  3.17s/it]07/12/2023 20:22:13 - INFO - __main__ - {'train_loss': 3.0161921993182723, 'epoch': 0, 'step': 314}
 69%|██████▉   | 315/455 [16:43<07:34,  3.25s/it]07/12/2023 20:22:16 - INFO - __main__ - {'train_loss': 3.0071378193204366, 'epoch': 0, 'step': 315}
 69%|██████▉   | 316/455 [16:46<07:30,  3.24s/it]07/12/2023 20:22:20 - INFO - __main__ - {'train_loss': 2.9992285619808148, 'epoch': 0, 'step': 316}
 70%|██████▉   | 317/455 [16:50<07:41,  3.34s/it]07/12/2023 20:22:23 - INFO - __main__ - {'train_loss': 2.9909071094982256, 'epoch': 0, 'step': 317}
 70%|██████▉   | 318/455 [16:53<07:39,  3.35s/it]07/12/2023 20:22:27 - INFO - __main__ - {'train_loss': 2.9836514071098663, 'epoch': 0, 'step': 318}
 70%|███████   | 319/455 [16:57<07:42,  3.40s/it]07/12/2023 20:22:30 - INFO - __main__ - {'train_loss': 2.975060537691027, 'epoch': 0, 'step': 319}
 70%|███████   | 320/455 [17:00<07:42,  3.43s/it]07/12/2023 20:22:34 - INFO - __main__ - {'train_loss': 2.966679000854492, 'epoch': 0, 'step': 320}
 71%|███████   | 321/455 [17:04<07:45,  3.48s/it]07/12/2023 20:22:37 - INFO - __main__ - {'train_loss': 2.957829460548092, 'epoch': 0, 'step': 321}
 71%|███████   | 322/455 [17:07<07:30,  3.38s/it]07/12/2023 20:22:40 - INFO - __main__ - {'train_loss': 2.9503420243352094, 'epoch': 0, 'step': 322}
 71%|███████   | 323/455 [17:11<07:30,  3.41s/it]07/12/2023 20:22:44 - INFO - __main__ - {'train_loss': 2.9416825143914473, 'epoch': 0, 'step': 323}
 71%|███████   | 324/455 [17:14<07:15,  3.33s/it]07/12/2023 20:22:47 - INFO - __main__ - {'train_loss': 2.933365810064622, 'epoch': 0, 'step': 324}
 71%|███████▏  | 325/455 [17:17<07:13,  3.34s/it]07/12/2023 20:22:50 - INFO - __main__ - {'train_loss': 2.924907977764423, 'epoch': 0, 'step': 325}
 72%|███████▏  | 326/455 [17:21<07:14,  3.37s/it]07/12/2023 20:22:54 - INFO - __main__ - {'train_loss': 2.9168045886455136, 'epoch': 0, 'step': 326}
 72%|███████▏  | 327/455 [17:24<07:07,  3.34s/it]07/12/2023 20:22:57 - INFO - __main__ - {'train_loss': 2.910309677823968, 'epoch': 0, 'step': 327}
 72%|███████▏  | 328/455 [17:27<06:58,  3.30s/it]07/12/2023 20:23:00 - INFO - __main__ - {'train_loss': 2.9034851818549923, 'epoch': 0, 'step': 328}
 72%|███████▏  | 329/455 [17:30<06:52,  3.28s/it]07/12/2023 20:23:03 - INFO - __main__ - {'train_loss': 2.894861841636588, 'epoch': 0, 'step': 329}
 73%|███████▎  | 330/455 [17:33<06:48,  3.27s/it]07/12/2023 20:23:07 - INFO - __main__ - {'train_loss': 2.8862389766808714, 'epoch': 0, 'step': 330}
 73%|███████▎  | 331/455 [17:37<06:42,  3.24s/it]07/12/2023 20:23:10 - INFO - __main__ - {'train_loss': 2.8789117818872736, 'epoch': 0, 'step': 331}
 73%|███████▎  | 332/455 [17:39<06:20,  3.09s/it]07/12/2023 20:23:13 - INFO - __main__ - {'train_loss': 2.8712286891707457, 'epoch': 0, 'step': 332}
 73%|███████▎  | 333/455 [17:42<06:07,  3.01s/it]07/12/2023 20:23:15 - INFO - __main__ - {'train_loss': 2.8631254545561187, 'epoch': 0, 'step': 333}
 73%|███████▎  | 334/455 [17:45<05:48,  2.88s/it]07/12/2023 20:23:18 - INFO - __main__ - {'train_loss': 2.854608775612837, 'epoch': 0, 'step': 334}
 74%|███████▎  | 335/455 [17:47<05:37,  2.81s/it]07/12/2023 20:23:21 - INFO - __main__ - {'train_loss': 2.8463630334654852, 'epoch': 0, 'step': 335}
 74%|███████▍  | 336/455 [17:50<05:33,  2.80s/it]07/12/2023 20:23:23 - INFO - __main__ - {'train_loss': 2.838903154645647, 'epoch': 0, 'step': 336}
 74%|███████▍  | 337/455 [17:53<05:22,  2.74s/it]07/12/2023 20:23:26 - INFO - __main__ - {'train_loss': 2.8316092561897257, 'epoch': 0, 'step': 337}
 74%|███████▍  | 338/455 [17:56<05:24,  2.77s/it]07/12/2023 20:23:29 - INFO - __main__ - {'train_loss': 2.8238355647882765, 'epoch': 0, 'step': 338}
 75%|███████▍  | 339/455 [17:58<05:14,  2.71s/it]07/12/2023 20:23:31 - INFO - __main__ - {'train_loss': 2.817199526986541, 'epoch': 0, 'step': 339}
 75%|███████▍  | 340/455 [18:01<05:19,  2.77s/it]07/12/2023 20:23:34 - INFO - __main__ - {'train_loss': 2.8093732048483457, 'epoch': 0, 'step': 340}
 75%|███████▍  | 341/455 [18:04<05:21,  2.82s/it]07/12/2023 20:23:37 - INFO - __main__ - {'train_loss': 2.8014925512050954, 'epoch': 0, 'step': 341}
 75%|███████▌  | 342/455 [18:07<05:14,  2.78s/it]07/12/2023 20:23:40 - INFO - __main__ - {'train_loss': 2.794157686289291, 'epoch': 0, 'step': 342}
 75%|███████▌  | 343/455 [18:10<05:10,  2.77s/it]07/12/2023 20:23:43 - INFO - __main__ - {'train_loss': 2.787173079332179, 'epoch': 0, 'step': 343}
 76%|███████▌  | 344/455 [18:12<05:10,  2.80s/it]07/12/2023 20:23:46 - INFO - __main__ - {'train_loss': 2.779661311659702, 'epoch': 0, 'step': 344}
 76%|███████▌  | 345/455 [18:16<05:25,  2.96s/it]07/12/2023 20:23:49 - INFO - __main__ - {'train_loss': 2.7733593042346016, 'epoch': 0, 'step': 345}
 76%|███████▌  | 346/455 [18:19<05:41,  3.13s/it]07/12/2023 20:23:52 - INFO - __main__ - {'train_loss': 2.7662187697570446, 'epoch': 0, 'step': 346}
 76%|███████▋  | 347/455 [18:23<05:46,  3.21s/it]07/12/2023 20:23:56 - INFO - __main__ - {'train_loss': 2.7601606825243157, 'epoch': 0, 'step': 347}
 76%|███████▋  | 348/455 [18:26<05:59,  3.36s/it]07/12/2023 20:24:00 - INFO - __main__ - {'train_loss': 2.7533348346578665, 'epoch': 0, 'step': 348}
 77%|███████▋  | 349/455 [18:30<05:54,  3.34s/it]07/12/2023 20:24:03 - INFO - __main__ - {'train_loss': 2.74727003212303, 'epoch': 0, 'step': 349}
 77%|███████▋  | 350/455 [18:33<05:51,  3.35s/it]07/12/2023 20:24:06 - INFO - __main__ - {'train_loss': 2.7414296177455357, 'epoch': 0, 'step': 350}
 77%|███████▋  | 351/455 [18:36<05:51,  3.38s/it]07/12/2023 20:24:10 - INFO - __main__ - {'train_loss': 2.7346041861422723, 'epoch': 0, 'step': 351}
 77%|███████▋  | 352/455 [18:40<05:42,  3.32s/it]07/12/2023 20:24:13 - INFO - __main__ - {'train_loss': 2.727003964510831, 'epoch': 0, 'step': 352}
 78%|███████▊  | 353/455 [18:43<05:36,  3.30s/it]07/12/2023 20:24:16 - INFO - __main__ - {'train_loss': 2.720230405121282, 'epoch': 0, 'step': 353}
 78%|███████▊  | 354/455 [18:46<05:37,  3.34s/it]07/12/2023 20:24:20 - INFO - __main__ - {'train_loss': 2.7137158065192444, 'epoch': 0, 'step': 354}
 78%|███████▊  | 355/455 [18:50<05:37,  3.37s/it]07/12/2023 20:24:23 - INFO - __main__ - {'train_loss': 2.7062572210607394, 'epoch': 0, 'step': 355}
 78%|███████▊  | 356/455 [18:53<05:33,  3.36s/it]07/12/2023 20:24:26 - INFO - __main__ - {'train_loss': 2.6998678485998946, 'epoch': 0, 'step': 356}
 78%|███████▊  | 357/455 [18:57<05:33,  3.41s/it]07/12/2023 20:24:30 - INFO - __main__ - {'train_loss': 2.6926334498643207, 'epoch': 0, 'step': 357}
 79%|███████▊  | 358/455 [19:00<05:34,  3.45s/it]07/12/2023 20:24:33 - INFO - __main__ - {'train_loss': 2.6869997845015714, 'epoch': 0, 'step': 358}
 79%|███████▉  | 359/455 [19:03<05:24,  3.38s/it]07/12/2023 20:24:37 - INFO - __main__ - {'train_loss': 2.6799040982982243, 'epoch': 0, 'step': 359}
 79%|███████▉  | 360/455 [19:07<05:17,  3.34s/it]07/12/2023 20:24:40 - INFO - __main__ - {'train_loss': 2.6740115695529516, 'epoch': 0, 'step': 360}
 79%|███████▉  | 361/455 [19:10<05:11,  3.32s/it]07/12/2023 20:24:43 - INFO - __main__ - {'train_loss': 2.667334728293802, 'epoch': 0, 'step': 361}
 80%|███████▉  | 362/455 [19:13<05:07,  3.31s/it]07/12/2023 20:24:46 - INFO - __main__ - {'train_loss': 2.6619411068068026, 'epoch': 0, 'step': 362}
 80%|███████▉  | 363/455 [19:17<05:09,  3.37s/it]07/12/2023 20:24:50 - INFO - __main__ - {'train_loss': 2.6572551464574725, 'epoch': 0, 'step': 363}
 80%|████████  | 364/455 [19:20<05:04,  3.35s/it]07/12/2023 20:24:53 - INFO - __main__ - {'train_loss': 2.6504566905262705, 'epoch': 0, 'step': 364}
 80%|████████  | 365/455 [19:24<05:04,  3.38s/it]07/12/2023 20:24:57 - INFO - __main__ - {'train_loss': 2.6433476696275684, 'epoch': 0, 'step': 365}
 80%|████████  | 366/455 [19:27<04:58,  3.35s/it]07/12/2023 20:25:00 - INFO - __main__ - {'train_loss': 2.6382279526340504, 'epoch': 0, 'step': 366}
 81%|████████  | 367/455 [19:30<04:55,  3.35s/it]07/12/2023 20:25:03 - INFO - __main__ - {'train_loss': 2.6327423178857288, 'epoch': 0, 'step': 367}
 81%|████████  | 368/455 [19:33<04:46,  3.29s/it]07/12/2023 20:25:07 - INFO - __main__ - {'train_loss': 2.626904363217561, 'epoch': 0, 'step': 368}
 81%|████████  | 369/455 [19:37<04:42,  3.28s/it]07/12/2023 20:25:10 - INFO - __main__ - {'train_loss': 2.620142660166836, 'epoch': 0, 'step': 369}
 81%|████████▏ | 370/455 [19:40<04:34,  3.22s/it]07/12/2023 20:25:13 - INFO - __main__ - {'train_loss': 2.6148665144636825, 'epoch': 0, 'step': 370}
 82%|████████▏ | 371/455 [19:43<04:33,  3.25s/it]07/12/2023 20:25:16 - INFO - __main__ - {'train_loss': 2.608226354552729, 'epoch': 0, 'step': 371}
 82%|████████▏ | 372/455 [19:46<04:27,  3.22s/it]07/12/2023 20:25:19 - INFO - __main__ - {'train_loss': 2.6028951009114585, 'epoch': 0, 'step': 372}
 82%|████████▏ | 373/455 [19:49<04:18,  3.15s/it]07/12/2023 20:25:22 - INFO - __main__ - {'train_loss': 2.596524236029658, 'epoch': 0, 'step': 373}
 82%|████████▏ | 374/455 [19:52<04:16,  3.17s/it]07/12/2023 20:25:26 - INFO - __main__ - {'train_loss': 2.5913258924841243, 'epoch': 0, 'step': 374}
 82%|████████▏ | 375/455 [19:56<04:16,  3.21s/it]07/12/2023 20:25:29 - INFO - __main__ - {'train_loss': 2.5846800130208334, 'epoch': 0, 'step': 375}
 83%|████████▎ | 376/455 [19:59<04:16,  3.24s/it]07/12/2023 20:25:32 - INFO - __main__ - {'train_loss': 2.5780383170919214, 'epoch': 0, 'step': 376}
 83%|████████▎ | 377/455 [20:02<04:08,  3.19s/it]07/12/2023 20:25:35 - INFO - __main__ - {'train_loss': 2.571645559619529, 'epoch': 0, 'step': 377}
 83%|████████▎ | 378/455 [20:05<04:06,  3.20s/it]07/12/2023 20:25:38 - INFO - __main__ - {'train_loss': 2.567963130890377, 'epoch': 0, 'step': 378}
 83%|████████▎ | 379/455 [20:09<04:11,  3.31s/it]07/12/2023 20:25:42 - INFO - __main__ - {'train_loss': 2.562651057985653, 'epoch': 0, 'step': 379}
 84%|████████▎ | 380/455 [20:12<04:10,  3.34s/it]07/12/2023 20:25:45 - INFO - __main__ - {'train_loss': 2.556791285464638, 'epoch': 0, 'step': 380}
 84%|████████▎ | 381/455 [20:16<04:07,  3.35s/it]07/12/2023 20:25:49 - INFO - __main__ - {'train_loss': 2.5521823352403215, 'epoch': 0, 'step': 381}
 84%|████████▍ | 382/455 [20:19<04:06,  3.38s/it]07/12/2023 20:25:52 - INFO - __main__ - {'train_loss': 2.546222147517179, 'epoch': 0, 'step': 382}
 84%|████████▍ | 383/455 [20:23<04:08,  3.45s/it]07/12/2023 20:25:56 - INFO - __main__ - {'train_loss': 2.540670449677709, 'epoch': 0, 'step': 383}
 84%|████████▍ | 384/455 [20:26<04:01,  3.41s/it]07/12/2023 20:25:59 - INFO - __main__ - {'train_loss': 2.5345627466837564, 'epoch': 0, 'step': 384}
 85%|████████▍ | 385/455 [20:29<03:56,  3.38s/it]07/12/2023 20:26:02 - INFO - __main__ - {'train_loss': 2.5285019911728894, 'epoch': 0, 'step': 385}
 85%|████████▍ | 386/455 [20:33<03:51,  3.36s/it]07/12/2023 20:26:06 - INFO - __main__ - {'train_loss': 2.5233884821284, 'epoch': 0, 'step': 386}
 85%|████████▌ | 387/455 [20:36<03:49,  3.38s/it]07/12/2023 20:26:09 - INFO - __main__ - {'train_loss': 2.5190382373425386, 'epoch': 0, 'step': 387}
 85%|████████▌ | 388/455 [20:39<03:46,  3.39s/it]07/12/2023 20:26:13 - INFO - __main__ - {'train_loss': 2.5127107285961663, 'epoch': 0, 'step': 388}
 85%|████████▌ | 389/455 [20:43<03:42,  3.37s/it]07/12/2023 20:26:16 - INFO - __main__ - {'train_loss': 2.506463450453888, 'epoch': 0, 'step': 389}
 86%|████████▌ | 390/455 [20:46<03:36,  3.34s/it]07/12/2023 20:26:19 - INFO - __main__ - {'train_loss': 2.5003145658052883, 'epoch': 0, 'step': 390}
 86%|████████▌ | 391/455 [20:49<03:33,  3.34s/it]07/12/2023 20:26:23 - INFO - __main__ - {'train_loss': 2.4952286430027173, 'epoch': 0, 'step': 391}
 86%|████████▌ | 392/455 [20:52<03:20,  3.19s/it]07/12/2023 20:26:25 - INFO - __main__ - {'train_loss': 2.4891435272839604, 'epoch': 0, 'step': 392}
 86%|████████▋ | 393/455 [20:55<03:19,  3.23s/it]07/12/2023 20:26:29 - INFO - __main__ - {'train_loss': 2.485056170980439, 'epoch': 0, 'step': 393}
 87%|████████▋ | 394/455 [20:59<03:17,  3.24s/it]07/12/2023 20:26:32 - INFO - __main__ - {'train_loss': 2.4805037600134834, 'epoch': 0, 'step': 394}
 87%|████████▋ | 395/455 [21:02<03:16,  3.28s/it]07/12/2023 20:26:35 - INFO - __main__ - {'train_loss': 2.475727168215981, 'epoch': 0, 'step': 395}
 87%|████████▋ | 396/455 [21:05<03:11,  3.25s/it]07/12/2023 20:26:39 - INFO - __main__ - {'train_loss': 2.4709549720841224, 'epoch': 0, 'step': 396}
 87%|████████▋ | 397/455 [21:08<03:04,  3.18s/it]07/12/2023 20:26:42 - INFO - __main__ - {'train_loss': 2.4659017952022984, 'epoch': 0, 'step': 397}
 87%|████████▋ | 398/455 [21:11<03:00,  3.17s/it]07/12/2023 20:26:45 - INFO - __main__ - {'train_loss': 2.4601428161314383, 'epoch': 0, 'step': 398}
 88%|████████▊ | 399/455 [21:15<02:56,  3.15s/it]07/12/2023 20:26:48 - INFO - __main__ - {'train_loss': 2.4551812269932642, 'epoch': 0, 'step': 399}
 88%|████████▊ | 400/455 [21:18<02:50,  3.10s/it]07/12/2023 20:26:51 - INFO - __main__ - {'train_loss': 2.4498196411132813, 'epoch': 0, 'step': 400}
 88%|████████▊ | 401/455 [21:21<02:47,  3.10s/it]07/12/2023 20:26:54 - INFO - __main__ - {'train_loss': 2.444884188454645, 'epoch': 0, 'step': 401}
 88%|████████▊ | 402/455 [21:24<02:46,  3.14s/it]07/12/2023 20:26:57 - INFO - __main__ - {'train_loss': 2.4394437116176926, 'epoch': 0, 'step': 402}
 89%|████████▊ | 403/455 [21:27<02:41,  3.11s/it]07/12/2023 20:27:00 - INFO - __main__ - {'train_loss': 2.4341368568742245, 'epoch': 0, 'step': 403}
 89%|████████▉ | 404/455 [21:30<02:40,  3.15s/it]07/12/2023 20:27:03 - INFO - __main__ - {'train_loss': 2.428423588818843, 'epoch': 0, 'step': 404}
 89%|████████▉ | 405/455 [21:34<02:41,  3.22s/it]07/12/2023 20:27:07 - INFO - __main__ - {'train_loss': 2.424173237364969, 'epoch': 0, 'step': 405}
 89%|████████▉ | 406/455 [21:37<02:37,  3.21s/it]07/12/2023 20:27:10 - INFO - __main__ - {'train_loss': 2.4202228414601294, 'epoch': 0, 'step': 406}
 89%|████████▉ | 407/455 [21:40<02:33,  3.21s/it]07/12/2023 20:27:13 - INFO - __main__ - {'train_loss': 2.414515089930129, 'epoch': 0, 'step': 407}
 90%|████████▉ | 408/455 [21:43<02:31,  3.23s/it]07/12/2023 20:27:16 - INFO - __main__ - {'train_loss': 2.410722620346967, 'epoch': 0, 'step': 408}
 90%|████████▉ | 409/455 [21:47<02:30,  3.26s/it]07/12/2023 20:27:20 - INFO - __main__ - {'train_loss': 2.405401477020935, 'epoch': 0, 'step': 409}
 90%|█████████ | 410/455 [21:50<02:28,  3.29s/it]07/12/2023 20:27:23 - INFO - __main__ - {'train_loss': 2.4008589581745428, 'epoch': 0, 'step': 410}
 90%|█████████ | 411/455 [21:53<02:23,  3.25s/it]07/12/2023 20:27:26 - INFO - __main__ - {'train_loss': 2.395369109736162, 'epoch': 0, 'step': 411}
 91%|█████████ | 412/455 [21:56<02:17,  3.19s/it]07/12/2023 20:27:29 - INFO - __main__ - {'train_loss': 2.3902851586202973, 'epoch': 0, 'step': 412}
 91%|█████████ | 413/455 [21:59<02:14,  3.21s/it]07/12/2023 20:27:33 - INFO - __main__ - {'train_loss': 2.3848404041502724, 'epoch': 0, 'step': 413}
 91%|█████████ | 414/455 [22:02<02:09,  3.17s/it]07/12/2023 20:27:36 - INFO - __main__ - {'train_loss': 2.380009305649909, 'epoch': 0, 'step': 414}
 91%|█████████ | 415/455 [22:05<02:04,  3.11s/it]07/12/2023 20:27:39 - INFO - __main__ - {'train_loss': 2.3749426416603914, 'epoch': 0, 'step': 415}
 91%|█████████▏| 416/455 [22:08<01:59,  3.07s/it]07/12/2023 20:27:42 - INFO - __main__ - {'train_loss': 2.3710271395169773, 'epoch': 0, 'step': 416}
 92%|█████████▏| 417/455 [22:11<01:54,  3.02s/it]07/12/2023 20:27:45 - INFO - __main__ - {'train_loss': 2.3656175645421165, 'epoch': 0, 'step': 417}
 92%|█████████▏| 418/455 [22:15<01:56,  3.14s/it]07/12/2023 20:27:48 - INFO - __main__ - {'train_loss': 2.3602665805360346, 'epoch': 0, 'step': 418}
 92%|█████████▏| 419/455 [22:18<01:50,  3.07s/it]07/12/2023 20:27:51 - INFO - __main__ - {'train_loss': 2.356264974735233, 'epoch': 0, 'step': 419}
 92%|█████████▏| 420/455 [22:21<01:49,  3.13s/it]07/12/2023 20:27:54 - INFO - __main__ - {'train_loss': 2.3515523274739585, 'epoch': 0, 'step': 420}
 93%|█████████▎| 421/455 [22:24<01:43,  3.03s/it]07/12/2023 20:27:57 - INFO - __main__ - {'train_loss': 2.3460409205203385, 'epoch': 0, 'step': 421}
 93%|█████████▎| 422/455 [22:27<01:41,  3.07s/it]07/12/2023 20:28:00 - INFO - __main__ - {'train_loss': 2.3426290936944607, 'epoch': 0, 'step': 422}
 93%|█████████▎| 423/455 [22:30<01:37,  3.06s/it]07/12/2023 20:28:03 - INFO - __main__ - {'train_loss': 2.3371556058843086, 'epoch': 0, 'step': 423}
 93%|█████████▎| 424/455 [22:33<01:34,  3.04s/it]07/12/2023 20:28:06 - INFO - __main__ - {'train_loss': 2.332251494785525, 'epoch': 0, 'step': 424}
 93%|█████████▎| 425/455 [22:36<01:29,  2.99s/it]07/12/2023 20:28:09 - INFO - __main__ - {'train_loss': 2.327301528033088, 'epoch': 0, 'step': 425}
 94%|█████████▎| 426/455 [22:39<01:27,  3.02s/it]07/12/2023 20:28:12 - INFO - __main__ - {'train_loss': 2.3226773973921655, 'epoch': 0, 'step': 426}
 94%|█████████▍| 427/455 [22:42<01:26,  3.07s/it]07/12/2023 20:28:15 - INFO - __main__ - {'train_loss': 2.317628954277664, 'epoch': 0, 'step': 427}
 94%|█████████▍| 428/455 [22:45<01:23,  3.11s/it]07/12/2023 20:28:18 - INFO - __main__ - {'train_loss': 2.3140849175854266, 'epoch': 0, 'step': 428}
 94%|█████████▍| 429/455 [22:48<01:21,  3.13s/it]07/12/2023 20:28:22 - INFO - __main__ - {'train_loss': 2.310147656705274, 'epoch': 0, 'step': 429}
 95%|█████████▍| 430/455 [22:52<01:19,  3.17s/it]07/12/2023 20:28:25 - INFO - __main__ - {'train_loss': 2.305283941224564, 'epoch': 0, 'step': 430}
 95%|█████████▍| 431/455 [22:55<01:16,  3.19s/it]07/12/2023 20:28:28 - INFO - __main__ - {'train_loss': 2.300044013286688, 'epoch': 0, 'step': 431}
 95%|█████████▍| 432/455 [22:58<01:13,  3.19s/it]07/12/2023 20:28:31 - INFO - __main__ - {'train_loss': 2.2969894409179688, 'epoch': 0, 'step': 432}
 95%|█████████▌| 433/455 [23:01<01:10,  3.18s/it]07/12/2023 20:28:35 - INFO - __main__ - {'train_loss': 2.292325132162962, 'epoch': 0, 'step': 433}
 95%|█████████▌| 434/455 [23:04<01:05,  3.10s/it]07/12/2023 20:28:37 - INFO - __main__ - {'train_loss': 2.2883933634252593, 'epoch': 0, 'step': 434}
 96%|█████████▌| 435/455 [23:07<01:01,  3.10s/it]07/12/2023 20:28:41 - INFO - __main__ - {'train_loss': 2.283752301095546, 'epoch': 0, 'step': 435}
 96%|█████████▌| 436/455 [23:11<01:02,  3.27s/it]07/12/2023 20:28:44 - INFO - __main__ - {'train_loss': 2.2791101298200975, 'epoch': 0, 'step': 436}
 96%|█████████▌| 437/455 [23:15<01:00,  3.36s/it]07/12/2023 20:28:48 - INFO - __main__ - {'train_loss': 2.2744288673662756, 'epoch': 0, 'step': 437}
 96%|█████████▋| 438/455 [23:18<00:56,  3.32s/it]07/12/2023 20:28:51 - INFO - __main__ - {'train_loss': 2.269927281767266, 'epoch': 0, 'step': 438}
 96%|█████████▋| 439/455 [23:21<00:53,  3.32s/it]07/12/2023 20:28:54 - INFO - __main__ - {'train_loss': 2.265383918084425, 'epoch': 0, 'step': 439}
 97%|█████████▋| 440/455 [23:24<00:49,  3.30s/it]07/12/2023 20:28:58 - INFO - __main__ - {'train_loss': 2.26205527565696, 'epoch': 0, 'step': 440}
 97%|█████████▋| 441/455 [23:28<00:45,  3.28s/it]07/12/2023 20:29:01 - INFO - __main__ - {'train_loss': 2.2569660351119616, 'epoch': 0, 'step': 441}
 97%|█████████▋| 442/455 [23:31<00:42,  3.25s/it]07/12/2023 20:29:04 - INFO - __main__ - {'train_loss': 2.252734706412613, 'epoch': 0, 'step': 442}
 97%|█████████▋| 443/455 [23:34<00:39,  3.30s/it]07/12/2023 20:29:07 - INFO - __main__ - {'train_loss': 2.248725288339447, 'epoch': 0, 'step': 443}
 98%|█████████▊| 444/455 [23:38<00:37,  3.37s/it]07/12/2023 20:29:11 - INFO - __main__ - {'train_loss': 2.244080689576295, 'epoch': 0, 'step': 444}
 98%|█████████▊| 445/455 [23:41<00:33,  3.36s/it]07/12/2023 20:29:14 - INFO - __main__ - {'train_loss': 2.239674223139045, 'epoch': 0, 'step': 445}
 98%|█████████▊| 446/455 [23:44<00:29,  3.25s/it]07/12/2023 20:29:17 - INFO - __main__ - {'train_loss': 2.2349546971342487, 'epoch': 0, 'step': 446}
 98%|█████████▊| 447/455 [23:47<00:25,  3.24s/it]07/12/2023 20:29:20 - INFO - __main__ - {'train_loss': 2.2308112022860738, 'epoch': 0, 'step': 447}
 98%|█████████▊| 448/455 [23:50<00:22,  3.21s/it]07/12/2023 20:29:24 - INFO - __main__ - {'train_loss': 2.227680206298828, 'epoch': 0, 'step': 448}
 99%|█████████▊| 449/455 [23:54<00:19,  3.21s/it]07/12/2023 20:29:27 - INFO - __main__ - {'train_loss': 2.22424126096186, 'epoch': 0, 'step': 449}
 99%|█████████▉| 450/455 [23:57<00:15,  3.15s/it]07/12/2023 20:29:30 - INFO - __main__ - {'train_loss': 2.2200927734375, 'epoch': 0, 'step': 450}
 99%|█████████▉| 451/455 [24:00<00:12,  3.15s/it]07/12/2023 20:29:33 - INFO - __main__ - {'train_loss': 2.217907146445399, 'epoch': 0, 'step': 451}
 99%|█████████▉| 452/455 [24:03<00:09,  3.18s/it]07/12/2023 20:29:36 - INFO - __main__ - {'train_loss': 2.2139206607784847, 'epoch': 0, 'step': 452}
100%|█████████▉| 453/455 [24:06<00:06,  3.19s/it]07/12/2023 20:29:39 - INFO - __main__ - {'train_loss': 2.210012136968819, 'epoch': 0, 'step': 453}
100%|█████████▉| 454/455 [24:09<00:03,  3.19s/it]07/12/2023 20:29:43 - INFO - __main__ - {'train_loss': 2.2070543734512666, 'epoch': 0, 'step': 454}
100%|██████████| 455/455 [24:12<00:00,  3.07s/it]07/12/2023 20:29:45 - INFO - __main__ - {'train_loss': 2.2023614569024725, 'epoch': 0, 'step': 455}
Generate config GenerationConfig {
  "_from_model_config": true,
  "decoder_start_token_id": 0,
  "eos_token_id": 1,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

07/12/2023 20:29:46 - INFO - absl - Using default tokenizer.
07/12/2023 20:29:47 - INFO - absl - Using default tokenizer.
07/12/2023 20:29:47 - INFO - __main__ - {'rouge1': 8.7719, 'rouge2': 5.2632, 'rougeL': 8.7719, 'rougeLsum': 8.7719}
07/12/2023 20:29:47 - INFO - absl - Using default tokenizer.
07/12/2023 20:29:47 - INFO - absl - Using default tokenizer.
Configuration saved in /home/wangran108/tmp/tst-summarization/config.json
Configuration saved in /home/wangran108/tmp/tst-summarization/generation_config.json
Model weights saved in /home/wangran108/tmp/tst-summarization/pytorch_model.bin
tokenizer config file saved in /home/wangran108/tmp/tst-summarization/tokenizer_config.json
Special tokens file saved in /home/wangran108/tmp/tst-summarization/special_tokens_map.json
Copy vocab file to /home/wangran108/tmp/tst-summarization/spiece.model
100%|██████████| 455/455 [24:24<00:00,  3.22s/it]
